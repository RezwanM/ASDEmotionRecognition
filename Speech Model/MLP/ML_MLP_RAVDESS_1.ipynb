{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************************************\n",
    "# Rezwan Matin\n",
    "# Thesis B\n",
    "# Filename: ML_MLP_RAVDESS_1.py\n",
    "# Date: 2/16/20\n",
    "#\n",
    "# Objective:\n",
    "# 26 MFCCs (mean) and 26 MFCCs (standard deviation), ZCR.\n",
    "#\n",
    "#*************************************************************************************\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as rosa\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.48219694e-322, 1.43370704e-315, 1.38338381e-322, ...,\n",
       "       9.32265673e+000, 9.21448291e-002, 8.00000000e+000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save directory path in 'path'\n",
    "path = r'C:\\Books\\Texas State Books\\Fall 2019\\Thesis A\\Corpus\\Simulated\\RAVDESS\\All'\n",
    "\n",
    "# Declare a dummy Numpy array (row vector)\n",
    "result_array = np.empty([1,54])\n",
    "\n",
    "# Create a list of audio file names 'file_list'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "i=0\n",
    "\n",
    "for filename in file_list:\n",
    "    \n",
    "    # Read WAV file. 'rosa.core.load' returns sampling frequency in 'fs' and audio signal in 'sig'\n",
    "    sig, fs = rosa.core.load(path + '\\\\' + file_list[i], sr=None)\n",
    "    \n",
    "    # Calculate the average mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    avg_mfcc_feat = np.mean(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26).T,axis=0)\n",
    "    \n",
    "    # Calculate the standard deviation of mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.std' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    std_mfcc_feat = np.std(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26).T,axis=0)\n",
    "    \n",
    "    # Calculate the average zero crossing rate (utterance-level feature) using 'rosa.feat.zero_crossing_rate()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    zcross_feat = rosa.feature.zero_crossing_rate(sig)\n",
    "    avg_zcross_feat = np.mean(rosa.feature.zero_crossing_rate(y=sig).T,axis=0)\n",
    "    \n",
    "    # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "    feat0 = np.append(avg_mfcc_feat, std_mfcc_feat, axis=0)\n",
    "    \n",
    "    feat1 = np.append(feat0, avg_zcross_feat, axis=0)\n",
    "    \n",
    "    # Save emotion label from file name. 'path' contains directory's address, 'file_list' contains file name, and '\\\\' joins the two to form file's address\n",
    "    label = os.path.splitext(os.path.basename(path + '\\\\' + file_list[i]))[0].split('-')[2]\n",
    "    \n",
    "    # Create a new Numpy array 'sample' to store features along with label\n",
    "    sample = np.insert(feat1, obj=53, values=label)\n",
    "    \n",
    "    result_array = np.append(result_array, sample)\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# Print out the 1D Numpy array\n",
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77814,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 54)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 1D Numpy array to 2D array\n",
    "result_array = np.reshape(result_array, (i+1,-1))\n",
    "\n",
    "# Delete first dummy row from 2D array\n",
    "result_array = np.delete(result_array, 0, 0)\n",
    "\n",
    "# Print final 2D Numpy array \n",
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>-596.489909</td>\n",
       "      <td>60.752540</td>\n",
       "      <td>1.096842</td>\n",
       "      <td>10.348159</td>\n",
       "      <td>-2.365429</td>\n",
       "      <td>7.986745</td>\n",
       "      <td>-2.027191</td>\n",
       "      <td>-0.116491</td>\n",
       "      <td>-6.654691</td>\n",
       "      <td>-1.406857</td>\n",
       "      <td>...</td>\n",
       "      <td>4.855516</td>\n",
       "      <td>7.889321</td>\n",
       "      <td>6.462217</td>\n",
       "      <td>6.308829</td>\n",
       "      <td>6.093730</td>\n",
       "      <td>4.775629</td>\n",
       "      <td>6.743428</td>\n",
       "      <td>4.953056</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>-625.565085</td>\n",
       "      <td>37.601565</td>\n",
       "      <td>-3.692515</td>\n",
       "      <td>8.979277</td>\n",
       "      <td>-5.637381</td>\n",
       "      <td>1.969832</td>\n",
       "      <td>-14.028974</td>\n",
       "      <td>1.621135</td>\n",
       "      <td>-13.249778</td>\n",
       "      <td>-2.752983</td>\n",
       "      <td>...</td>\n",
       "      <td>8.569099</td>\n",
       "      <td>9.995390</td>\n",
       "      <td>9.139846</td>\n",
       "      <td>7.003860</td>\n",
       "      <td>8.466257</td>\n",
       "      <td>6.507719</td>\n",
       "      <td>6.422495</td>\n",
       "      <td>7.017977</td>\n",
       "      <td>0.100822</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>-589.096402</td>\n",
       "      <td>81.593995</td>\n",
       "      <td>-0.673410</td>\n",
       "      <td>12.050125</td>\n",
       "      <td>10.394698</td>\n",
       "      <td>19.574634</td>\n",
       "      <td>-4.826030</td>\n",
       "      <td>2.336833</td>\n",
       "      <td>-1.092758</td>\n",
       "      <td>-2.186526</td>\n",
       "      <td>...</td>\n",
       "      <td>7.340301</td>\n",
       "      <td>6.230392</td>\n",
       "      <td>6.443201</td>\n",
       "      <td>8.798983</td>\n",
       "      <td>4.902457</td>\n",
       "      <td>4.589814</td>\n",
       "      <td>7.527595</td>\n",
       "      <td>4.950332</td>\n",
       "      <td>0.060662</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>-567.861314</td>\n",
       "      <td>50.748331</td>\n",
       "      <td>-6.996325</td>\n",
       "      <td>2.507971</td>\n",
       "      <td>-4.691477</td>\n",
       "      <td>0.188368</td>\n",
       "      <td>-15.399068</td>\n",
       "      <td>-8.173095</td>\n",
       "      <td>-10.797020</td>\n",
       "      <td>-5.656966</td>\n",
       "      <td>...</td>\n",
       "      <td>11.171980</td>\n",
       "      <td>8.206702</td>\n",
       "      <td>7.901195</td>\n",
       "      <td>8.828436</td>\n",
       "      <td>9.154752</td>\n",
       "      <td>9.932455</td>\n",
       "      <td>9.488486</td>\n",
       "      <td>9.067261</td>\n",
       "      <td>0.071194</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>-493.306198</td>\n",
       "      <td>48.233703</td>\n",
       "      <td>-3.028220</td>\n",
       "      <td>18.852342</td>\n",
       "      <td>-3.590502</td>\n",
       "      <td>9.721251</td>\n",
       "      <td>-2.724754</td>\n",
       "      <td>6.184319</td>\n",
       "      <td>-1.879220</td>\n",
       "      <td>2.796708</td>\n",
       "      <td>...</td>\n",
       "      <td>8.423227</td>\n",
       "      <td>8.431807</td>\n",
       "      <td>10.100727</td>\n",
       "      <td>6.233350</td>\n",
       "      <td>5.368571</td>\n",
       "      <td>6.040829</td>\n",
       "      <td>8.945960</td>\n",
       "      <td>9.681306</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>-615.406134</td>\n",
       "      <td>43.642912</td>\n",
       "      <td>-6.402692</td>\n",
       "      <td>12.012579</td>\n",
       "      <td>-11.143824</td>\n",
       "      <td>5.057568</td>\n",
       "      <td>-10.034647</td>\n",
       "      <td>9.767619</td>\n",
       "      <td>-5.317049</td>\n",
       "      <td>-4.267572</td>\n",
       "      <td>...</td>\n",
       "      <td>8.939621</td>\n",
       "      <td>6.847688</td>\n",
       "      <td>8.352500</td>\n",
       "      <td>6.104008</td>\n",
       "      <td>9.562205</td>\n",
       "      <td>9.682800</td>\n",
       "      <td>7.298084</td>\n",
       "      <td>11.122962</td>\n",
       "      <td>0.062682</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>-696.193632</td>\n",
       "      <td>72.680527</td>\n",
       "      <td>6.986117</td>\n",
       "      <td>24.891391</td>\n",
       "      <td>2.902952</td>\n",
       "      <td>9.812980</td>\n",
       "      <td>1.004795</td>\n",
       "      <td>9.548923</td>\n",
       "      <td>-5.149150</td>\n",
       "      <td>-1.495172</td>\n",
       "      <td>...</td>\n",
       "      <td>6.992092</td>\n",
       "      <td>7.291586</td>\n",
       "      <td>6.982269</td>\n",
       "      <td>7.322814</td>\n",
       "      <td>5.810611</td>\n",
       "      <td>5.193900</td>\n",
       "      <td>8.329027</td>\n",
       "      <td>4.884544</td>\n",
       "      <td>0.053795</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>-615.786554</td>\n",
       "      <td>53.243328</td>\n",
       "      <td>-18.664904</td>\n",
       "      <td>4.902306</td>\n",
       "      <td>-5.074936</td>\n",
       "      <td>2.717509</td>\n",
       "      <td>-12.459353</td>\n",
       "      <td>0.465174</td>\n",
       "      <td>-15.718720</td>\n",
       "      <td>-9.009732</td>\n",
       "      <td>...</td>\n",
       "      <td>8.659487</td>\n",
       "      <td>9.041492</td>\n",
       "      <td>9.361286</td>\n",
       "      <td>8.517360</td>\n",
       "      <td>8.430611</td>\n",
       "      <td>9.607357</td>\n",
       "      <td>7.680336</td>\n",
       "      <td>10.634757</td>\n",
       "      <td>0.073165</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>-540.863667</td>\n",
       "      <td>61.330114</td>\n",
       "      <td>-9.318194</td>\n",
       "      <td>14.311654</td>\n",
       "      <td>0.700296</td>\n",
       "      <td>10.042510</td>\n",
       "      <td>-8.720129</td>\n",
       "      <td>-2.253887</td>\n",
       "      <td>-3.264636</td>\n",
       "      <td>-8.769154</td>\n",
       "      <td>...</td>\n",
       "      <td>6.450383</td>\n",
       "      <td>7.456291</td>\n",
       "      <td>5.409208</td>\n",
       "      <td>5.574703</td>\n",
       "      <td>5.130295</td>\n",
       "      <td>6.740843</td>\n",
       "      <td>6.362919</td>\n",
       "      <td>4.742207</td>\n",
       "      <td>0.069006</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>-560.186503</td>\n",
       "      <td>44.504401</td>\n",
       "      <td>-14.639301</td>\n",
       "      <td>2.251491</td>\n",
       "      <td>-11.031190</td>\n",
       "      <td>5.346806</td>\n",
       "      <td>-10.163466</td>\n",
       "      <td>-6.553192</td>\n",
       "      <td>-9.599661</td>\n",
       "      <td>-7.118306</td>\n",
       "      <td>...</td>\n",
       "      <td>8.714345</td>\n",
       "      <td>12.087737</td>\n",
       "      <td>11.294306</td>\n",
       "      <td>11.975909</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>10.152889</td>\n",
       "      <td>10.284631</td>\n",
       "      <td>8.092669</td>\n",
       "      <td>0.083883</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>-549.112537</td>\n",
       "      <td>51.645593</td>\n",
       "      <td>-13.533260</td>\n",
       "      <td>5.057537</td>\n",
       "      <td>-7.751437</td>\n",
       "      <td>6.483627</td>\n",
       "      <td>-8.185426</td>\n",
       "      <td>-3.678960</td>\n",
       "      <td>-7.495473</td>\n",
       "      <td>-5.939384</td>\n",
       "      <td>...</td>\n",
       "      <td>6.832755</td>\n",
       "      <td>7.636967</td>\n",
       "      <td>9.260758</td>\n",
       "      <td>6.784180</td>\n",
       "      <td>6.211125</td>\n",
       "      <td>5.630223</td>\n",
       "      <td>8.386614</td>\n",
       "      <td>10.673716</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>-536.693773</td>\n",
       "      <td>43.281145</td>\n",
       "      <td>-9.283843</td>\n",
       "      <td>9.252940</td>\n",
       "      <td>-10.546879</td>\n",
       "      <td>3.863054</td>\n",
       "      <td>-11.571782</td>\n",
       "      <td>-4.176649</td>\n",
       "      <td>-9.068328</td>\n",
       "      <td>-4.904865</td>\n",
       "      <td>...</td>\n",
       "      <td>10.236763</td>\n",
       "      <td>9.565846</td>\n",
       "      <td>11.509971</td>\n",
       "      <td>10.812383</td>\n",
       "      <td>9.733490</td>\n",
       "      <td>10.543449</td>\n",
       "      <td>9.538634</td>\n",
       "      <td>9.322657</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "1236 -596.489909  60.752540   1.096842  10.348159  -2.365429   7.986745   \n",
       "1237 -625.565085  37.601565  -3.692515   8.979277  -5.637381   1.969832   \n",
       "1238 -589.096402  81.593995  -0.673410  12.050125  10.394698  19.574634   \n",
       "1239 -567.861314  50.748331  -6.996325   2.507971  -4.691477   0.188368   \n",
       "1240 -493.306198  48.233703  -3.028220  18.852342  -3.590502   9.721251   \n",
       "1241 -615.406134  43.642912  -6.402692  12.012579 -11.143824   5.057568   \n",
       "1242 -696.193632  72.680527   6.986117  24.891391   2.902952   9.812980   \n",
       "1243 -615.786554  53.243328 -18.664904   4.902306  -5.074936   2.717509   \n",
       "1244 -540.863667  61.330114  -9.318194  14.311654   0.700296  10.042510   \n",
       "1245 -560.186503  44.504401 -14.639301   2.251491 -11.031190   5.346806   \n",
       "1246 -549.112537  51.645593 -13.533260   5.057537  -7.751437   6.483627   \n",
       "1247 -536.693773  43.281145  -9.283843   9.252940 -10.546879   3.863054   \n",
       "\n",
       "              6         7          8         9  ...         44         45  \\\n",
       "1236  -2.027191 -0.116491  -6.654691 -1.406857  ...   4.855516   7.889321   \n",
       "1237 -14.028974  1.621135 -13.249778 -2.752983  ...   8.569099   9.995390   \n",
       "1238  -4.826030  2.336833  -1.092758 -2.186526  ...   7.340301   6.230392   \n",
       "1239 -15.399068 -8.173095 -10.797020 -5.656966  ...  11.171980   8.206702   \n",
       "1240  -2.724754  6.184319  -1.879220  2.796708  ...   8.423227   8.431807   \n",
       "1241 -10.034647  9.767619  -5.317049 -4.267572  ...   8.939621   6.847688   \n",
       "1242   1.004795  9.548923  -5.149150 -1.495172  ...   6.992092   7.291586   \n",
       "1243 -12.459353  0.465174 -15.718720 -9.009732  ...   8.659487   9.041492   \n",
       "1244  -8.720129 -2.253887  -3.264636 -8.769154  ...   6.450383   7.456291   \n",
       "1245 -10.163466 -6.553192  -9.599661 -7.118306  ...   8.714345  12.087737   \n",
       "1246  -8.185426 -3.678960  -7.495473 -5.939384  ...   6.832755   7.636967   \n",
       "1247 -11.571782 -4.176649  -9.068328 -4.904865  ...  10.236763   9.565846   \n",
       "\n",
       "             46         47         48         49         50         51  \\\n",
       "1236   6.462217   6.308829   6.093730   4.775629   6.743428   4.953056   \n",
       "1237   9.139846   7.003860   8.466257   6.507719   6.422495   7.017977   \n",
       "1238   6.443201   8.798983   4.902457   4.589814   7.527595   4.950332   \n",
       "1239   7.901195   8.828436   9.154752   9.932455   9.488486   9.067261   \n",
       "1240  10.100727   6.233350   5.368571   6.040829   8.945960   9.681306   \n",
       "1241   8.352500   6.104008   9.562205   9.682800   7.298084  11.122962   \n",
       "1242   6.982269   7.322814   5.810611   5.193900   8.329027   4.884544   \n",
       "1243   9.361286   8.517360   8.430611   9.607357   7.680336  10.634757   \n",
       "1244   5.409208   5.574703   5.130295   6.740843   6.362919   4.742207   \n",
       "1245  11.294306  11.975909  15.346170  10.152889  10.284631   8.092669   \n",
       "1246   9.260758   6.784180   6.211125   5.630223   8.386614  10.673716   \n",
       "1247  11.509971  10.812383   9.733490  10.543449   9.538634   9.322657   \n",
       "\n",
       "            52  Emotion  \n",
       "1236  0.066450      8.0  \n",
       "1237  0.100822      8.0  \n",
       "1238  0.060662      8.0  \n",
       "1239  0.071194      8.0  \n",
       "1240  0.068974      8.0  \n",
       "1241  0.062682      8.0  \n",
       "1242  0.053795      8.0  \n",
       "1243  0.073165      8.0  \n",
       "1244  0.069006      8.0  \n",
       "1245  0.083883      8.0  \n",
       "1246  0.075679      8.0  \n",
       "1247  0.092145      8.0  \n",
       "\n",
       "[12 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=result_array)\n",
    "# Label only the last (target) column\n",
    "df = df.rename({53: \"Emotion\"}, axis='columns')\n",
    "# Delete unnecessary emotion data (calm)\n",
    "df.drop(df[df['Emotion'] == 2.0].index, inplace = True)\n",
    "# Reset row (sample) indexing\n",
    "df = df.reset_index(drop=True)\n",
    "df.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0    192\n",
       "7.0    192\n",
       "6.0    192\n",
       "5.0    192\n",
       "4.0    192\n",
       "3.0    192\n",
       "1.0     96\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0    192\n",
       "7.0    192\n",
       "6.0    192\n",
       "5.0    192\n",
       "4.0    192\n",
       "3.0    192\n",
       "1.0    192\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the dataset for equal number of samples for each class.\n",
    "# Separate majority and minority classes\n",
    "df_minority = df[df.Emotion==1.0]\n",
    "df_majority3 = df[df.Emotion==3.0]\n",
    "df_majority4 = df[df.Emotion==4.0]\n",
    "df_majority5 = df[df.Emotion==5.0]\n",
    "df_majority6 = df[df.Emotion==6.0]\n",
    "df_majority7 = df[df.Emotion==7.0]\n",
    "df_majority8 = df[df.Emotion==8.0]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=192,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_minority_upsampled, df_majority3, df_majority4, df_majority5, df_majority6, df_majority7, df_majority8])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.Emotion.value_counts()\n",
    "\n",
    "# Reset row (sample) indexing\n",
    "df_upsampled = df_upsampled.reset_index(drop=True)\n",
    "\n",
    "df_upsampled['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 8. 8. 8.]\n"
     ]
    }
   ],
   "source": [
    "# Extract target feature 'Emotion' in a vector y. Indexing from 0\n",
    "y = df_upsampled.iloc[0:1344, 53].values\n",
    "# Extract features 'buying' and 'safety' in a vector X. Indexing from 0\n",
    "X = df_upsampled.iloc[0:1344, list(range(53))].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 53) (1008,)\n",
      "(336, 53) (336,)\n"
     ]
    }
   ],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "# Standardize the inputs\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# One-Hot Encode the classes\n",
    "y_train_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Maleeha\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Create an object/instance 'model' for the 'Sequential()' class.\n",
    "model = keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=X_train_centered.shape[1],\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros', \n",
    "                activation='relu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='relu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='relu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='relu'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense( units=y_train_onehot.shape[1],\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "                    lr=0.001, decay=1e-7, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "                    loss='categorical_crossentropy')\n",
    "                          \n",
    "                            # cross-entropy: fancy name for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 907 samples, validate on 101 samples\n",
      "Epoch 1/50\n",
      "907/907 [==============================] - 1s 805us/sample - loss: 2.1953 - val_loss: 2.1899\n",
      "Epoch 2/50\n",
      "907/907 [==============================] - 0s 262us/sample - loss: 2.1874 - val_loss: 2.1802\n",
      "Epoch 3/50\n",
      "907/907 [==============================] - 0s 97us/sample - loss: 2.1777 - val_loss: 2.1705\n",
      "Epoch 4/50\n",
      "907/907 [==============================] - 0s 71us/sample - loss: 2.1679 - val_loss: 2.1614\n",
      "Epoch 5/50\n",
      "907/907 [==============================] - 0s 64us/sample - loss: 2.1585 - val_loss: 2.1524\n",
      "Epoch 6/50\n",
      "907/907 [==============================] - 0s 62us/sample - loss: 2.1494 - val_loss: 2.1435\n",
      "Epoch 7/50\n",
      "907/907 [==============================] - 0s 66us/sample - loss: 2.1406 - val_loss: 2.1353\n",
      "Epoch 8/50\n",
      "907/907 [==============================] - 0s 93us/sample - loss: 2.1321 - val_loss: 2.1271\n",
      "Epoch 9/50\n",
      "907/907 [==============================] - 0s 71us/sample - loss: 2.1237 - val_loss: 2.1186\n",
      "Epoch 10/50\n",
      "907/907 [==============================] - 0s 62us/sample - loss: 2.1154 - val_loss: 2.1104\n",
      "Epoch 11/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 2.1072 - val_loss: 2.1021\n",
      "Epoch 12/50\n",
      "907/907 [==============================] - 0s 103us/sample - loss: 2.0989 - val_loss: 2.0939\n",
      "Epoch 13/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 2.0907 - val_loss: 2.0854\n",
      "Epoch 14/50\n",
      "907/907 [==============================] - 0s 76us/sample - loss: 2.0823 - val_loss: 2.0771\n",
      "Epoch 15/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 2.0740 - val_loss: 2.0685\n",
      "Epoch 16/50\n",
      "907/907 [==============================] - 0s 57us/sample - loss: 2.0658 - val_loss: 2.0600\n",
      "Epoch 17/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 2.0575 - val_loss: 2.0515\n",
      "Epoch 18/50\n",
      "907/907 [==============================] - 0s 82us/sample - loss: 2.0492 - val_loss: 2.0431\n",
      "Epoch 19/50\n",
      "907/907 [==============================] - 0s 64us/sample - loss: 2.0406 - val_loss: 2.0343\n",
      "Epoch 20/50\n",
      "907/907 [==============================] - 0s 73us/sample - loss: 2.0318 - val_loss: 2.0254\n",
      "Epoch 21/50\n",
      "907/907 [==============================] - 0s 104us/sample - loss: 2.0231 - val_loss: 2.0158\n",
      "Epoch 22/50\n",
      "907/907 [==============================] - 0s 109us/sample - loss: 2.0140 - val_loss: 2.0064\n",
      "Epoch 23/50\n",
      "907/907 [==============================] - 0s 68us/sample - loss: 2.0047 - val_loss: 1.9965\n",
      "Epoch 24/50\n",
      "907/907 [==============================] - 0s 62us/sample - loss: 1.9954 - val_loss: 1.9866\n",
      "Epoch 25/50\n",
      "907/907 [==============================] - 0s 80us/sample - loss: 1.9861 - val_loss: 1.9762\n",
      "Epoch 26/50\n",
      "907/907 [==============================] - 0s 79us/sample - loss: 1.9763 - val_loss: 1.9660\n",
      "Epoch 27/50\n",
      "907/907 [==============================] - 0s 67us/sample - loss: 1.9664 - val_loss: 1.9552\n",
      "Epoch 28/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.9561 - val_loss: 1.9435\n",
      "Epoch 29/50\n",
      "907/907 [==============================] - 0s 71us/sample - loss: 1.9458 - val_loss: 1.9319\n",
      "Epoch 30/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.9354 - val_loss: 1.9204\n",
      "Epoch 31/50\n",
      "907/907 [==============================] - 0s 83us/sample - loss: 1.9250 - val_loss: 1.9094\n",
      "Epoch 32/50\n",
      "907/907 [==============================] - 0s 100us/sample - loss: 1.9147 - val_loss: 1.8979\n",
      "Epoch 33/50\n",
      "907/907 [==============================] - 0s 69us/sample - loss: 1.9042 - val_loss: 1.8863\n",
      "Epoch 34/50\n",
      "907/907 [==============================] - 0s 98us/sample - loss: 1.8935 - val_loss: 1.8755\n",
      "Epoch 35/50\n",
      "907/907 [==============================] - 0s 110us/sample - loss: 1.8833 - val_loss: 1.8629\n",
      "Epoch 36/50\n",
      "907/907 [==============================] - 0s 75us/sample - loss: 1.8727 - val_loss: 1.8521\n",
      "Epoch 37/50\n",
      "907/907 [==============================] - 0s 69us/sample - loss: 1.8626 - val_loss: 1.8418\n",
      "Epoch 38/50\n",
      "907/907 [==============================] - 0s 68us/sample - loss: 1.8529 - val_loss: 1.8317\n",
      "Epoch 39/50\n",
      "907/907 [==============================] - 0s 78us/sample - loss: 1.8435 - val_loss: 1.8220\n",
      "Epoch 40/50\n",
      "907/907 [==============================] - 0s 85us/sample - loss: 1.8347 - val_loss: 1.8111\n",
      "Epoch 41/50\n",
      "907/907 [==============================] - 0s 83us/sample - loss: 1.8256 - val_loss: 1.8021\n",
      "Epoch 42/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 1.8174 - val_loss: 1.7937\n",
      "Epoch 43/50\n",
      "907/907 [==============================] - 0s 89us/sample - loss: 1.8094 - val_loss: 1.7856\n",
      "Epoch 44/50\n",
      "907/907 [==============================] - 0s 104us/sample - loss: 1.8023 - val_loss: 1.7788\n",
      "Epoch 45/50\n",
      "907/907 [==============================] - 0s 111us/sample - loss: 1.7954 - val_loss: 1.7709\n",
      "Epoch 46/50\n",
      "907/907 [==============================] - 0s 76us/sample - loss: 1.7891 - val_loss: 1.7648\n",
      "Epoch 47/50\n",
      "907/907 [==============================] - 0s 72us/sample - loss: 1.7836 - val_loss: 1.7600\n",
      "Epoch 48/50\n",
      "907/907 [==============================] - 0s 137us/sample - loss: 1.7779 - val_loss: 1.7541\n",
      "Epoch 49/50\n",
      "907/907 [==============================] - 0s 111us/sample - loss: 1.7729 - val_loss: 1.7478\n",
      "Epoch 50/50\n",
      "907/907 [==============================] - 0s 105us/sample - loss: 1.7680 - val_loss: 1.7411\n"
     ]
    }
   ],
   "source": [
    "# Train the MLP\n",
    "history = model.fit(X_train_centered, y_train_onehot, batch_size=64, epochs=50, verbose=1, validation_split=0.1) # 90% training / 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 28.87%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 30.95%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Loss vs. Epochs curve\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Save plot as PNG file\n",
    "fig.savefig('Loss vs Epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
