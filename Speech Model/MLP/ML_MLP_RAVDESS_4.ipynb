{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************************************\n",
    "# Rezwan Matin\n",
    "# Thesis B\n",
    "# Filename: ML_MLP_RAVDESS_4.py\n",
    "# Date: 3/20/20\n",
    "#\n",
    "# Objective:\n",
    "# 26 MFCCs (mean) and 26 MFCCs (standard deviation), ZCR with Loss Function\n",
    "#\n",
    "#*************************************************************************************\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as rosa\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 9.32265673, 0.09214483,\n",
       "       8.        ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save directory path in 'path'\n",
    "path = r'C:/Books/Texas State Books/Fall 2019/Thesis A/Corpus/Simulated/RAVDESS/All'\n",
    "\n",
    "# Declare a dummy Numpy array (row vector)\n",
    "result_array = np.empty([1,54])\n",
    "\n",
    "# Create a list of audio file names 'file_list'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "i=0\n",
    "\n",
    "for filename in file_list:\n",
    "    \n",
    "    # Read WAV file. 'rosa.core.load' returns sampling frequency in 'fs' and audio signal in 'sig'\n",
    "    sig, fs = rosa.core.load(path + '/' + file_list[i], sr=None)\n",
    "    \n",
    "    # Calculate the average mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    avg_mfcc_feat = np.mean(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26).T,axis=0)\n",
    "    \n",
    "    # Calculate the standard deviation of mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.std' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    std_mfcc_feat = np.std(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26).T,axis=0)\n",
    "    \n",
    "    # Calculate the average zero crossing rate (utterance-level feature) using 'rosa.feat.zero_crossing_rate()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    zcross_feat = rosa.feature.zero_crossing_rate(sig)\n",
    "    avg_zcross_feat = np.mean(rosa.feature.zero_crossing_rate(y=sig).T,axis=0)\n",
    "    \n",
    "    # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "    feat0 = np.append(avg_mfcc_feat, std_mfcc_feat, axis=0)\n",
    "    \n",
    "    feat1 = np.append(feat0, avg_zcross_feat, axis=0)\n",
    "    \n",
    "    # Save emotion label from file name. 'path' contains directory's address, 'file_list' contains file name, and '\\\\' joins the two to form file's address\n",
    "    label = os.path.splitext(os.path.basename(path + '\\\\' + file_list[i]))[0].split('-')[2]\n",
    "    \n",
    "    # Create a new Numpy array 'sample' to store features along with label\n",
    "    sample = np.insert(feat1, obj=53, values=label)\n",
    "    \n",
    "    result_array = np.append(result_array, sample)\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# Print out the 1D Numpy array\n",
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 54)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_array.shape\n",
    "\n",
    "# Convert 1D Numpy array to 2D array. Argument must be a Tuple. i+1 because we have i samples (audio files) plus a dummy row.\n",
    "result_array = np.reshape(result_array, (i+1,-1))\n",
    "\n",
    "# Delete first dummy row from 2D array\n",
    "result_array = np.delete(result_array, 0, 0)\n",
    "\n",
    "# Print final 2D Numpy array \n",
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>-596.489909</td>\n",
       "      <td>60.752540</td>\n",
       "      <td>1.096842</td>\n",
       "      <td>10.348159</td>\n",
       "      <td>-2.365429</td>\n",
       "      <td>7.986745</td>\n",
       "      <td>-2.027191</td>\n",
       "      <td>-0.116491</td>\n",
       "      <td>-6.654691</td>\n",
       "      <td>-1.406857</td>\n",
       "      <td>...</td>\n",
       "      <td>4.855516</td>\n",
       "      <td>7.889321</td>\n",
       "      <td>6.462217</td>\n",
       "      <td>6.308829</td>\n",
       "      <td>6.093730</td>\n",
       "      <td>4.775629</td>\n",
       "      <td>6.743428</td>\n",
       "      <td>4.953056</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>-625.565085</td>\n",
       "      <td>37.601565</td>\n",
       "      <td>-3.692515</td>\n",
       "      <td>8.979277</td>\n",
       "      <td>-5.637381</td>\n",
       "      <td>1.969832</td>\n",
       "      <td>-14.028974</td>\n",
       "      <td>1.621135</td>\n",
       "      <td>-13.249778</td>\n",
       "      <td>-2.752983</td>\n",
       "      <td>...</td>\n",
       "      <td>8.569099</td>\n",
       "      <td>9.995390</td>\n",
       "      <td>9.139846</td>\n",
       "      <td>7.003860</td>\n",
       "      <td>8.466257</td>\n",
       "      <td>6.507719</td>\n",
       "      <td>6.422495</td>\n",
       "      <td>7.017977</td>\n",
       "      <td>0.100822</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>-589.096402</td>\n",
       "      <td>81.593995</td>\n",
       "      <td>-0.673410</td>\n",
       "      <td>12.050125</td>\n",
       "      <td>10.394698</td>\n",
       "      <td>19.574634</td>\n",
       "      <td>-4.826030</td>\n",
       "      <td>2.336833</td>\n",
       "      <td>-1.092758</td>\n",
       "      <td>-2.186526</td>\n",
       "      <td>...</td>\n",
       "      <td>7.340301</td>\n",
       "      <td>6.230392</td>\n",
       "      <td>6.443201</td>\n",
       "      <td>8.798983</td>\n",
       "      <td>4.902457</td>\n",
       "      <td>4.589814</td>\n",
       "      <td>7.527595</td>\n",
       "      <td>4.950332</td>\n",
       "      <td>0.060662</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>-567.861314</td>\n",
       "      <td>50.748331</td>\n",
       "      <td>-6.996325</td>\n",
       "      <td>2.507971</td>\n",
       "      <td>-4.691477</td>\n",
       "      <td>0.188368</td>\n",
       "      <td>-15.399068</td>\n",
       "      <td>-8.173095</td>\n",
       "      <td>-10.797020</td>\n",
       "      <td>-5.656966</td>\n",
       "      <td>...</td>\n",
       "      <td>11.171980</td>\n",
       "      <td>8.206702</td>\n",
       "      <td>7.901195</td>\n",
       "      <td>8.828436</td>\n",
       "      <td>9.154752</td>\n",
       "      <td>9.932455</td>\n",
       "      <td>9.488486</td>\n",
       "      <td>9.067261</td>\n",
       "      <td>0.071194</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>-493.306198</td>\n",
       "      <td>48.233703</td>\n",
       "      <td>-3.028220</td>\n",
       "      <td>18.852342</td>\n",
       "      <td>-3.590502</td>\n",
       "      <td>9.721251</td>\n",
       "      <td>-2.724754</td>\n",
       "      <td>6.184319</td>\n",
       "      <td>-1.879220</td>\n",
       "      <td>2.796708</td>\n",
       "      <td>...</td>\n",
       "      <td>8.423227</td>\n",
       "      <td>8.431807</td>\n",
       "      <td>10.100727</td>\n",
       "      <td>6.233350</td>\n",
       "      <td>5.368571</td>\n",
       "      <td>6.040829</td>\n",
       "      <td>8.945960</td>\n",
       "      <td>9.681306</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>-615.406134</td>\n",
       "      <td>43.642912</td>\n",
       "      <td>-6.402692</td>\n",
       "      <td>12.012579</td>\n",
       "      <td>-11.143824</td>\n",
       "      <td>5.057568</td>\n",
       "      <td>-10.034647</td>\n",
       "      <td>9.767619</td>\n",
       "      <td>-5.317049</td>\n",
       "      <td>-4.267572</td>\n",
       "      <td>...</td>\n",
       "      <td>8.939621</td>\n",
       "      <td>6.847688</td>\n",
       "      <td>8.352500</td>\n",
       "      <td>6.104008</td>\n",
       "      <td>9.562205</td>\n",
       "      <td>9.682800</td>\n",
       "      <td>7.298084</td>\n",
       "      <td>11.122962</td>\n",
       "      <td>0.062682</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>-696.193632</td>\n",
       "      <td>72.680527</td>\n",
       "      <td>6.986117</td>\n",
       "      <td>24.891391</td>\n",
       "      <td>2.902952</td>\n",
       "      <td>9.812980</td>\n",
       "      <td>1.004795</td>\n",
       "      <td>9.548923</td>\n",
       "      <td>-5.149150</td>\n",
       "      <td>-1.495172</td>\n",
       "      <td>...</td>\n",
       "      <td>6.992092</td>\n",
       "      <td>7.291586</td>\n",
       "      <td>6.982269</td>\n",
       "      <td>7.322814</td>\n",
       "      <td>5.810611</td>\n",
       "      <td>5.193900</td>\n",
       "      <td>8.329027</td>\n",
       "      <td>4.884544</td>\n",
       "      <td>0.053795</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>-615.786554</td>\n",
       "      <td>53.243328</td>\n",
       "      <td>-18.664904</td>\n",
       "      <td>4.902306</td>\n",
       "      <td>-5.074936</td>\n",
       "      <td>2.717509</td>\n",
       "      <td>-12.459353</td>\n",
       "      <td>0.465174</td>\n",
       "      <td>-15.718720</td>\n",
       "      <td>-9.009732</td>\n",
       "      <td>...</td>\n",
       "      <td>8.659487</td>\n",
       "      <td>9.041492</td>\n",
       "      <td>9.361286</td>\n",
       "      <td>8.517360</td>\n",
       "      <td>8.430611</td>\n",
       "      <td>9.607357</td>\n",
       "      <td>7.680336</td>\n",
       "      <td>10.634757</td>\n",
       "      <td>0.073165</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>-540.863667</td>\n",
       "      <td>61.330114</td>\n",
       "      <td>-9.318194</td>\n",
       "      <td>14.311654</td>\n",
       "      <td>0.700296</td>\n",
       "      <td>10.042510</td>\n",
       "      <td>-8.720129</td>\n",
       "      <td>-2.253887</td>\n",
       "      <td>-3.264636</td>\n",
       "      <td>-8.769154</td>\n",
       "      <td>...</td>\n",
       "      <td>6.450383</td>\n",
       "      <td>7.456291</td>\n",
       "      <td>5.409208</td>\n",
       "      <td>5.574703</td>\n",
       "      <td>5.130295</td>\n",
       "      <td>6.740843</td>\n",
       "      <td>6.362919</td>\n",
       "      <td>4.742207</td>\n",
       "      <td>0.069006</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>-560.186503</td>\n",
       "      <td>44.504401</td>\n",
       "      <td>-14.639301</td>\n",
       "      <td>2.251491</td>\n",
       "      <td>-11.031190</td>\n",
       "      <td>5.346806</td>\n",
       "      <td>-10.163466</td>\n",
       "      <td>-6.553192</td>\n",
       "      <td>-9.599661</td>\n",
       "      <td>-7.118306</td>\n",
       "      <td>...</td>\n",
       "      <td>8.714345</td>\n",
       "      <td>12.087737</td>\n",
       "      <td>11.294306</td>\n",
       "      <td>11.975909</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>10.152889</td>\n",
       "      <td>10.284631</td>\n",
       "      <td>8.092669</td>\n",
       "      <td>0.083883</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>-549.112537</td>\n",
       "      <td>51.645593</td>\n",
       "      <td>-13.533260</td>\n",
       "      <td>5.057537</td>\n",
       "      <td>-7.751437</td>\n",
       "      <td>6.483627</td>\n",
       "      <td>-8.185426</td>\n",
       "      <td>-3.678960</td>\n",
       "      <td>-7.495473</td>\n",
       "      <td>-5.939384</td>\n",
       "      <td>...</td>\n",
       "      <td>6.832755</td>\n",
       "      <td>7.636967</td>\n",
       "      <td>9.260758</td>\n",
       "      <td>6.784180</td>\n",
       "      <td>6.211125</td>\n",
       "      <td>5.630223</td>\n",
       "      <td>8.386614</td>\n",
       "      <td>10.673716</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>-536.693773</td>\n",
       "      <td>43.281145</td>\n",
       "      <td>-9.283843</td>\n",
       "      <td>9.252940</td>\n",
       "      <td>-10.546879</td>\n",
       "      <td>3.863054</td>\n",
       "      <td>-11.571782</td>\n",
       "      <td>-4.176649</td>\n",
       "      <td>-9.068328</td>\n",
       "      <td>-4.904865</td>\n",
       "      <td>...</td>\n",
       "      <td>10.236763</td>\n",
       "      <td>9.565846</td>\n",
       "      <td>11.509971</td>\n",
       "      <td>10.812383</td>\n",
       "      <td>9.733490</td>\n",
       "      <td>10.543449</td>\n",
       "      <td>9.538634</td>\n",
       "      <td>9.322657</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "1236 -596.489909  60.752540   1.096842  10.348159  -2.365429   7.986745   \n",
       "1237 -625.565085  37.601565  -3.692515   8.979277  -5.637381   1.969832   \n",
       "1238 -589.096402  81.593995  -0.673410  12.050125  10.394698  19.574634   \n",
       "1239 -567.861314  50.748331  -6.996325   2.507971  -4.691477   0.188368   \n",
       "1240 -493.306198  48.233703  -3.028220  18.852342  -3.590502   9.721251   \n",
       "1241 -615.406134  43.642912  -6.402692  12.012579 -11.143824   5.057568   \n",
       "1242 -696.193632  72.680527   6.986117  24.891391   2.902952   9.812980   \n",
       "1243 -615.786554  53.243328 -18.664904   4.902306  -5.074936   2.717509   \n",
       "1244 -540.863667  61.330114  -9.318194  14.311654   0.700296  10.042510   \n",
       "1245 -560.186503  44.504401 -14.639301   2.251491 -11.031190   5.346806   \n",
       "1246 -549.112537  51.645593 -13.533260   5.057537  -7.751437   6.483627   \n",
       "1247 -536.693773  43.281145  -9.283843   9.252940 -10.546879   3.863054   \n",
       "\n",
       "              6         7          8         9  ...         44         45  \\\n",
       "1236  -2.027191 -0.116491  -6.654691 -1.406857  ...   4.855516   7.889321   \n",
       "1237 -14.028974  1.621135 -13.249778 -2.752983  ...   8.569099   9.995390   \n",
       "1238  -4.826030  2.336833  -1.092758 -2.186526  ...   7.340301   6.230392   \n",
       "1239 -15.399068 -8.173095 -10.797020 -5.656966  ...  11.171980   8.206702   \n",
       "1240  -2.724754  6.184319  -1.879220  2.796708  ...   8.423227   8.431807   \n",
       "1241 -10.034647  9.767619  -5.317049 -4.267572  ...   8.939621   6.847688   \n",
       "1242   1.004795  9.548923  -5.149150 -1.495172  ...   6.992092   7.291586   \n",
       "1243 -12.459353  0.465174 -15.718720 -9.009732  ...   8.659487   9.041492   \n",
       "1244  -8.720129 -2.253887  -3.264636 -8.769154  ...   6.450383   7.456291   \n",
       "1245 -10.163466 -6.553192  -9.599661 -7.118306  ...   8.714345  12.087737   \n",
       "1246  -8.185426 -3.678960  -7.495473 -5.939384  ...   6.832755   7.636967   \n",
       "1247 -11.571782 -4.176649  -9.068328 -4.904865  ...  10.236763   9.565846   \n",
       "\n",
       "             46         47         48         49         50         51  \\\n",
       "1236   6.462217   6.308829   6.093730   4.775629   6.743428   4.953056   \n",
       "1237   9.139846   7.003860   8.466257   6.507719   6.422495   7.017977   \n",
       "1238   6.443201   8.798983   4.902457   4.589814   7.527595   4.950332   \n",
       "1239   7.901195   8.828436   9.154752   9.932455   9.488486   9.067261   \n",
       "1240  10.100727   6.233350   5.368571   6.040829   8.945960   9.681306   \n",
       "1241   8.352500   6.104008   9.562205   9.682800   7.298084  11.122962   \n",
       "1242   6.982269   7.322814   5.810611   5.193900   8.329027   4.884544   \n",
       "1243   9.361286   8.517360   8.430611   9.607357   7.680336  10.634757   \n",
       "1244   5.409208   5.574703   5.130295   6.740843   6.362919   4.742207   \n",
       "1245  11.294306  11.975909  15.346170  10.152889  10.284631   8.092669   \n",
       "1246   9.260758   6.784180   6.211125   5.630223   8.386614  10.673716   \n",
       "1247  11.509971  10.812383   9.733490  10.543449   9.538634   9.322657   \n",
       "\n",
       "            52  Emotion  \n",
       "1236  0.066450      8.0  \n",
       "1237  0.100822      8.0  \n",
       "1238  0.060662      8.0  \n",
       "1239  0.071194      8.0  \n",
       "1240  0.068974      8.0  \n",
       "1241  0.062682      8.0  \n",
       "1242  0.053795      8.0  \n",
       "1243  0.073165      8.0  \n",
       "1244  0.069006      8.0  \n",
       "1245  0.083883      8.0  \n",
       "1246  0.075679      8.0  \n",
       "1247  0.092145      8.0  \n",
       "\n",
       "[12 rows x 54 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=result_array)\n",
    "# Label only the last (target) column\n",
    "df = df.rename({53: \"Emotion\"}, axis='columns')\n",
    "# Delete unnecessary emotion data (calm)\n",
    "df.drop(df[df['Emotion'] == 2.0].index, inplace = True)\n",
    "# Reset row (sample) indexing\n",
    "df = df.reset_index(drop=True)\n",
    "df.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0    192\n",
       "7.0    192\n",
       "6.0    192\n",
       "5.0    192\n",
       "4.0    192\n",
       "3.0    192\n",
       "1.0    192\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()\n",
    "\n",
    "# Balance the dataset for equal number of samples for each class.\n",
    "# Separate majority and minority classes\n",
    "df_minority = df[df.Emotion==1.0]\n",
    "df_majority3 = df[df.Emotion==3.0]\n",
    "df_majority4 = df[df.Emotion==4.0]\n",
    "df_majority5 = df[df.Emotion==5.0]\n",
    "df_majority6 = df[df.Emotion==6.0]\n",
    "df_majority7 = df[df.Emotion==7.0]\n",
    "df_majority8 = df[df.Emotion==8.0]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=192,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_minority_upsampled, df_majority3, df_majority4, df_majority5, df_majority6, df_majority7, df_majority8])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.Emotion.value_counts()\n",
    "\n",
    "# Reset row (sample) indexing\n",
    "df_upsampled = df_upsampled.reset_index(drop=True)\n",
    "\n",
    "df_upsampled['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 8. 8. 8.]\n"
     ]
    }
   ],
   "source": [
    "# Extract target feature 'Emotion' in a vector y. Indexing from 0\n",
    "y = df_upsampled.iloc[0:1344, 53].values\n",
    "# Extract features 'buying' and 'safety' in a vector X. Indexing from 0\n",
    "X = df_upsampled.iloc[0:1344, list(range(53))].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 53) (1008,)\n",
      "(336, 53) (336,)\n"
     ]
    }
   ],
   "source": [
    "# Split data for training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\n",
    "\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "# Standardize the inputs\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# One-Hot Encode the classes\n",
    "#y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "\n",
    "# learning rate scheduler which reduces learning rate to half every 10 epochs\n",
    "def scheduler(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object/instance 'model' for the 'Sequential()' class.\n",
    "model = keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=X_train_centered.shape[1],\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros', \n",
    "                activation='selu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=90,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='selu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=90,\n",
    "                input_dim=90,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='selu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=90,\n",
    "                input_dim=90,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='selu'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense( units=y_train.shape[0],\n",
    "                input_dim=90,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "                    lr=0.0, decay=1e-7, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "                    loss='sparse_categorical_crossentropy')\n",
    "                          \n",
    "                            # cross-entropy: fancy name for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 907 samples, validate on 101 samples\n",
      "Epoch 1/200\n",
      "907/907 [==============================] - 1s 1ms/sample - loss: 3.6133 - val_loss: 1.9202\n",
      "Epoch 2/200\n",
      "907/907 [==============================] - 0s 404us/sample - loss: 1.8690 - val_loss: 1.7181\n",
      "Epoch 3/200\n",
      "907/907 [==============================] - 0s 396us/sample - loss: 1.7280 - val_loss: 1.7689\n",
      "Epoch 4/200\n",
      "907/907 [==============================] - 0s 459us/sample - loss: 1.7952 - val_loss: 1.6136\n",
      "Epoch 5/200\n",
      "907/907 [==============================] - 0s 376us/sample - loss: 1.6149 - val_loss: 1.5863\n",
      "Epoch 6/200\n",
      "907/907 [==============================] - 0s 369us/sample - loss: 1.6473 - val_loss: 1.5143\n",
      "Epoch 7/200\n",
      "907/907 [==============================] - 1s 1ms/sample - loss: 1.5550 - val_loss: 1.4996\n",
      "Epoch 8/200\n",
      "907/907 [==============================] - 0s 372us/sample - loss: 1.5745 - val_loss: 1.7055\n",
      "Epoch 9/200\n",
      "907/907 [==============================] - 0s 374us/sample - loss: 1.5266 - val_loss: 1.5015\n",
      "Epoch 10/200\n",
      "907/907 [==============================] - 0s 405us/sample - loss: 1.4816 - val_loss: 1.4069\n",
      "Epoch 11/200\n",
      "907/907 [==============================] - 0s 368us/sample - loss: 1.3667 - val_loss: 1.4120\n",
      "Epoch 12/200\n",
      "907/907 [==============================] - 0s 396us/sample - loss: 1.3871 - val_loss: 1.3913\n",
      "Epoch 13/200\n",
      "907/907 [==============================] - 0s 393us/sample - loss: 1.3262 - val_loss: 1.3941\n",
      "Epoch 14/200\n",
      "907/907 [==============================] - 0s 394us/sample - loss: 1.3897 - val_loss: 1.3766\n",
      "Epoch 15/200\n",
      "907/907 [==============================] - 0s 396us/sample - loss: 1.3548 - val_loss: 1.3752\n",
      "Epoch 16/200\n",
      "907/907 [==============================] - 0s 394us/sample - loss: 1.3624 - val_loss: 1.3547\n",
      "Epoch 17/200\n",
      "907/907 [==============================] - 0s 416us/sample - loss: 1.3322 - val_loss: 1.3085\n",
      "Epoch 18/200\n",
      "907/907 [==============================] - 0s 396us/sample - loss: 1.2975 - val_loss: 1.3180\n",
      "Epoch 19/200\n",
      "907/907 [==============================] - 0s 389us/sample - loss: 1.2844 - val_loss: 1.2822\n",
      "Epoch 20/200\n",
      "907/907 [==============================] - 0s 401us/sample - loss: 1.2163 - val_loss: 1.2705\n",
      "Epoch 21/200\n",
      "907/907 [==============================] - 0s 398us/sample - loss: 1.2193 - val_loss: 1.2725\n",
      "Epoch 22/200\n",
      "907/907 [==============================] - 0s 416us/sample - loss: 1.1798 - val_loss: 1.2828\n",
      "Epoch 23/200\n",
      "907/907 [==============================] - 0s 418us/sample - loss: 1.1780 - val_loss: 1.3240\n",
      "Epoch 24/200\n",
      "907/907 [==============================] - 0s 394us/sample - loss: 1.1688 - val_loss: 1.3450\n",
      "Epoch 25/200\n",
      "907/907 [==============================] - 0s 390us/sample - loss: 1.1432 - val_loss: 1.2309\n",
      "Epoch 26/200\n",
      "907/907 [==============================] - 0s 394us/sample - loss: 1.1503 - val_loss: 1.2524\n",
      "Epoch 27/200\n",
      "907/907 [==============================] - 0s 401us/sample - loss: 1.1789 - val_loss: 1.2189\n",
      "Epoch 28/200\n",
      "907/907 [==============================] - 0s 405us/sample - loss: 1.1685 - val_loss: 1.3581\n",
      "Epoch 29/200\n",
      "907/907 [==============================] - 0s 401us/sample - loss: 1.1340 - val_loss: 1.3149\n",
      "Epoch 30/200\n",
      "907/907 [==============================] - 0s 400us/sample - loss: 1.1054 - val_loss: 1.1875\n",
      "Epoch 31/200\n",
      "907/907 [==============================] - 0s 401us/sample - loss: 1.0664 - val_loss: 1.2324\n",
      "Epoch 32/200\n",
      "907/907 [==============================] - 0s 398us/sample - loss: 1.0601 - val_loss: 1.1888\n",
      "Epoch 33/200\n",
      "907/907 [==============================] - 0s 398us/sample - loss: 1.0499 - val_loss: 1.1837\n",
      "Epoch 34/200\n",
      "907/907 [==============================] - 0s 405us/sample - loss: 1.0551 - val_loss: 1.1673\n",
      "Epoch 35/200\n",
      "907/907 [==============================] - 0s 412us/sample - loss: 1.0332 - val_loss: 1.2707\n",
      "Epoch 36/200\n",
      "907/907 [==============================] - 0s 405us/sample - loss: 1.0345 - val_loss: 1.1955\n",
      "Epoch 37/200\n",
      "907/907 [==============================] - 0s 410us/sample - loss: 1.0249 - val_loss: 1.1887\n",
      "Epoch 38/200\n",
      "907/907 [==============================] - 0s 401us/sample - loss: 1.0365 - val_loss: 1.1521\n",
      "Epoch 39/200\n",
      "907/907 [==============================] - 0s 400us/sample - loss: 1.0559 - val_loss: 1.2070\n",
      "Epoch 40/200\n",
      "907/907 [==============================] - 0s 413us/sample - loss: 1.0056 - val_loss: 1.1659\n",
      "Epoch 41/200\n",
      "907/907 [==============================] - 0s 399us/sample - loss: 0.9859 - val_loss: 1.1743\n",
      "Epoch 42/200\n",
      "907/907 [==============================] - 0s 405us/sample - loss: 0.9817 - val_loss: 1.1464\n",
      "Epoch 43/200\n",
      "907/907 [==============================] - 0s 404us/sample - loss: 0.9735 - val_loss: 1.2062\n",
      "Epoch 44/200\n",
      "907/907 [==============================] - 0s 400us/sample - loss: 0.9787 - val_loss: 1.1730\n",
      "Epoch 45/200\n",
      "907/907 [==============================] - 0s 394us/sample - loss: 0.9726 - val_loss: 1.1404\n",
      "Epoch 46/200\n",
      "907/907 [==============================] - 0s 404us/sample - loss: 0.9690 - val_loss: 1.1627\n",
      "Epoch 47/200\n",
      "907/907 [==============================] - 0s 415us/sample - loss: 0.9670 - val_loss: 1.1897\n",
      "Epoch 48/200\n",
      "907/907 [==============================] - 0s 431us/sample - loss: 0.9765 - val_loss: 1.1375\n",
      "Epoch 49/200\n",
      "907/907 [==============================] - 1s 1ms/sample - loss: 0.9607 - val_loss: 1.1404\n",
      "Epoch 50/200\n",
      "907/907 [==============================] - 0s 411us/sample - loss: 0.9449 - val_loss: 1.1334\n",
      "Epoch 51/200\n",
      "907/907 [==============================] - 0s 410us/sample - loss: 0.9361 - val_loss: 1.1306\n",
      "Epoch 52/200\n",
      "907/907 [==============================] - 0s 405us/sample - loss: 0.9308 - val_loss: 1.1211\n",
      "Epoch 53/200\n",
      "907/907 [==============================] - 0s 456us/sample - loss: 0.9440 - val_loss: 1.1452\n",
      "Epoch 54/200\n",
      "907/907 [==============================] - 0s 412us/sample - loss: 0.9293 - val_loss: 1.1534\n",
      "Epoch 55/200\n",
      "907/907 [==============================] - 0s 480us/sample - loss: 0.9297 - val_loss: 1.1239\n",
      "Epoch 56/200\n",
      "907/907 [==============================] - 0s 420us/sample - loss: 0.9338 - val_loss: 1.1628\n",
      "Epoch 57/200\n",
      "907/907 [==============================] - 1s 576us/sample - loss: 0.9270 - val_loss: 1.1337\n",
      "Epoch 58/200\n",
      "907/907 [==============================] - 0s 432us/sample - loss: 0.9214 - val_loss: 1.1701\n",
      "Epoch 59/200\n",
      "907/907 [==============================] - 0s 425us/sample - loss: 0.9228 - val_loss: 1.1100\n",
      "Epoch 60/200\n",
      "907/907 [==============================] - 0s 460us/sample - loss: 0.9175 - val_loss: 1.1228\n",
      "Epoch 61/200\n",
      "907/907 [==============================] - 0s 440us/sample - loss: 0.9134 - val_loss: 1.1149\n",
      "Epoch 62/200\n",
      "907/907 [==============================] - 0s 427us/sample - loss: 0.9117 - val_loss: 1.1143\n",
      "Epoch 63/200\n",
      "907/907 [==============================] - 0s 428us/sample - loss: 0.9095 - val_loss: 1.1357\n",
      "Epoch 64/200\n",
      "907/907 [==============================] - 0s 431us/sample - loss: 0.9097 - val_loss: 1.1152\n",
      "Epoch 65/200\n",
      "907/907 [==============================] - 0s 432us/sample - loss: 0.9071 - val_loss: 1.1203\n",
      "Epoch 66/200\n",
      "907/907 [==============================] - 0s 449us/sample - loss: 0.9058 - val_loss: 1.1153\n",
      "Epoch 67/200\n",
      "907/907 [==============================] - 0s 417us/sample - loss: 0.9066 - val_loss: 1.1280\n",
      "Epoch 68/200\n",
      "907/907 [==============================] - 0s 445us/sample - loss: 0.9050 - val_loss: 1.1261\n",
      "Epoch 69/200\n",
      "907/907 [==============================] - 0s 432us/sample - loss: 0.9053 - val_loss: 1.1233\n",
      "Epoch 70/200\n",
      "907/907 [==============================] - 0s 439us/sample - loss: 0.8991 - val_loss: 1.1137\n",
      "Epoch 71/200\n",
      "907/907 [==============================] - 0s 432us/sample - loss: 0.8974 - val_loss: 1.1156\n",
      "Epoch 72/200\n",
      "907/907 [==============================] - 0s 433us/sample - loss: 0.8953 - val_loss: 1.1132\n",
      "Epoch 73/200\n",
      "907/907 [==============================] - 0s 519us/sample - loss: 0.8969 - val_loss: 1.1095\n",
      "Epoch 74/200\n",
      "907/907 [==============================] - 0s 413us/sample - loss: 0.8967 - val_loss: 1.1210\n",
      "Epoch 75/200\n",
      "907/907 [==============================] - 0s 422us/sample - loss: 0.8974 - val_loss: 1.1168\n",
      "Epoch 76/200\n",
      "907/907 [==============================] - 0s 450us/sample - loss: 0.8941 - val_loss: 1.1111\n",
      "Epoch 77/200\n",
      "907/907 [==============================] - 0s 418us/sample - loss: 0.8943 - val_loss: 1.1110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8935 - val_loss: 1.1148\n",
      "Epoch 79/200\n",
      "907/907 [==============================] - 0s 347us/sample - loss: 0.8929 - val_loss: 1.1065\n",
      "Epoch 80/200\n",
      "907/907 [==============================] - 0s 355us/sample - loss: 0.8914 - val_loss: 1.1113\n",
      "Epoch 81/200\n",
      "907/907 [==============================] - 0s 347us/sample - loss: 0.8896 - val_loss: 1.1102\n",
      "Epoch 82/200\n",
      "907/907 [==============================] - 0s 356us/sample - loss: 0.8896 - val_loss: 1.1062\n",
      "Epoch 83/200\n",
      "907/907 [==============================] - 0s 347us/sample - loss: 0.8892 - val_loss: 1.1076\n",
      "Epoch 84/200\n",
      "907/907 [==============================] - 0s 350us/sample - loss: 0.8892 - val_loss: 1.1068\n",
      "Epoch 85/200\n",
      "907/907 [==============================] - 0s 353us/sample - loss: 0.8888 - val_loss: 1.1121\n",
      "Epoch 86/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8881 - val_loss: 1.1102\n",
      "Epoch 87/200\n",
      "907/907 [==============================] - 0s 351us/sample - loss: 0.8869 - val_loss: 1.1106\n",
      "Epoch 88/200\n",
      "907/907 [==============================] - 0s 353us/sample - loss: 0.8869 - val_loss: 1.1097\n",
      "Epoch 89/200\n",
      "907/907 [==============================] - 1s 1ms/sample - loss: 0.8885 - val_loss: 1.1062\n",
      "Epoch 90/200\n",
      "907/907 [==============================] - 0s 356us/sample - loss: 0.8861 - val_loss: 1.1097\n",
      "Epoch 91/200\n",
      "907/907 [==============================] - 0s 356us/sample - loss: 0.8861 - val_loss: 1.1112\n",
      "Epoch 92/200\n",
      "907/907 [==============================] - 0s 361us/sample - loss: 0.8852 - val_loss: 1.1109\n",
      "Epoch 93/200\n",
      "907/907 [==============================] - 0s 356us/sample - loss: 0.8850 - val_loss: 1.1097\n",
      "Epoch 94/200\n",
      "907/907 [==============================] - 0s 362us/sample - loss: 0.8849 - val_loss: 1.1092\n",
      "Epoch 95/200\n",
      "907/907 [==============================] - 0s 398us/sample - loss: 0.8852 - val_loss: 1.1094\n",
      "Epoch 96/200\n",
      "907/907 [==============================] - 0s 364us/sample - loss: 0.8848 - val_loss: 1.1097\n",
      "Epoch 97/200\n",
      "907/907 [==============================] - 0s 356us/sample - loss: 0.8846 - val_loss: 1.1095\n",
      "Epoch 98/200\n",
      "907/907 [==============================] - 0s 363us/sample - loss: 0.8848 - val_loss: 1.1083\n",
      "Epoch 99/200\n",
      "907/907 [==============================] - 0s 365us/sample - loss: 0.8845 - val_loss: 1.1087\n",
      "Epoch 100/200\n",
      "907/907 [==============================] - 0s 364us/sample - loss: 0.8836 - val_loss: 1.1099\n",
      "Epoch 101/200\n",
      "907/907 [==============================] - 0s 366us/sample - loss: 0.8835 - val_loss: 1.1086\n",
      "Epoch 102/200\n",
      "907/907 [==============================] - 0s 367us/sample - loss: 0.8836 - val_loss: 1.1094\n",
      "Epoch 103/200\n",
      "907/907 [==============================] - 0s 363us/sample - loss: 0.8836 - val_loss: 1.1101\n",
      "Epoch 104/200\n",
      "907/907 [==============================] - 0s 363us/sample - loss: 0.8834 - val_loss: 1.1102\n",
      "Epoch 105/200\n",
      "907/907 [==============================] - 0s 367us/sample - loss: 0.8833 - val_loss: 1.1095\n",
      "Epoch 106/200\n",
      "907/907 [==============================] - 0s 369us/sample - loss: 0.8836 - val_loss: 1.1095\n",
      "Epoch 107/200\n",
      "907/907 [==============================] - 0s 380us/sample - loss: 0.8831 - val_loss: 1.1084\n",
      "Epoch 108/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8830 - val_loss: 1.1088\n",
      "Epoch 109/200\n",
      "907/907 [==============================] - 0s 366us/sample - loss: 0.8830 - val_loss: 1.1095\n",
      "Epoch 110/200\n",
      "907/907 [==============================] - 0s 372us/sample - loss: 0.8827 - val_loss: 1.1087\n",
      "Epoch 111/200\n",
      "907/907 [==============================] - 0s 393us/sample - loss: 0.8828 - val_loss: 1.1093\n",
      "Epoch 112/200\n",
      "907/907 [==============================] - 0s 364us/sample - loss: 0.8827 - val_loss: 1.1089\n",
      "Epoch 113/200\n",
      "907/907 [==============================] - 0s 361us/sample - loss: 0.8826 - val_loss: 1.1093\n",
      "Epoch 114/200\n",
      "907/907 [==============================] - 0s 390us/sample - loss: 0.8825 - val_loss: 1.1092\n",
      "Epoch 115/200\n",
      "907/907 [==============================] - 0s 370us/sample - loss: 0.8824 - val_loss: 1.1092\n",
      "Epoch 116/200\n",
      "907/907 [==============================] - 0s 372us/sample - loss: 0.8825 - val_loss: 1.1087\n",
      "Epoch 117/200\n",
      "907/907 [==============================] - 0s 368us/sample - loss: 0.8826 - val_loss: 1.1091\n",
      "Epoch 118/200\n",
      "907/907 [==============================] - 0s 364us/sample - loss: 0.8824 - val_loss: 1.1085\n",
      "Epoch 119/200\n",
      "907/907 [==============================] - 0s 364us/sample - loss: 0.8823 - val_loss: 1.1089\n",
      "Epoch 120/200\n",
      "907/907 [==============================] - 0s 369us/sample - loss: 0.8822 - val_loss: 1.1087\n",
      "Epoch 121/200\n",
      "907/907 [==============================] - 0s 381us/sample - loss: 0.8821 - val_loss: 1.1090\n",
      "Epoch 122/200\n",
      "907/907 [==============================] - 0s 367us/sample - loss: 0.8821 - val_loss: 1.1091\n",
      "Epoch 123/200\n",
      "907/907 [==============================] - 0s 384us/sample - loss: 0.8822 - val_loss: 1.1088\n",
      "Epoch 124/200\n",
      "907/907 [==============================] - 0s 367us/sample - loss: 0.8820 - val_loss: 1.1091\n",
      "Epoch 125/200\n",
      "907/907 [==============================] - 0s 376us/sample - loss: 0.8820 - val_loss: 1.1089\n",
      "Epoch 126/200\n",
      "907/907 [==============================] - 0s 372us/sample - loss: 0.8821 - val_loss: 1.1090\n",
      "Epoch 127/200\n",
      "907/907 [==============================] - 0s 368us/sample - loss: 0.8820 - val_loss: 1.1087\n",
      "Epoch 128/200\n",
      "907/907 [==============================] - 0s 387us/sample - loss: 0.8820 - val_loss: 1.1087\n",
      "Epoch 129/200\n",
      "907/907 [==============================] - 0s 363us/sample - loss: 0.8820 - val_loss: 1.1088\n",
      "Epoch 130/200\n",
      "907/907 [==============================] - 0s 409us/sample - loss: 0.8819 - val_loss: 1.1089\n",
      "Epoch 131/200\n",
      "907/907 [==============================] - 0s 374us/sample - loss: 0.8819 - val_loss: 1.1088\n",
      "Epoch 132/200\n",
      "907/907 [==============================] - 0s 380us/sample - loss: 0.8819 - val_loss: 1.1088\n",
      "Epoch 133/200\n",
      "907/907 [==============================] - 0s 377us/sample - loss: 0.8819 - val_loss: 1.1088\n",
      "Epoch 134/200\n",
      "907/907 [==============================] - 1s 581us/sample - loss: 0.8819 - val_loss: 1.1087\n",
      "Epoch 135/200\n",
      "907/907 [==============================] - 1s 832us/sample - loss: 0.8819 - val_loss: 1.1087\n",
      "Epoch 136/200\n",
      "907/907 [==============================] - 0s 408us/sample - loss: 0.8818 - val_loss: 1.1089\n",
      "Epoch 137/200\n",
      "907/907 [==============================] - 0s 395us/sample - loss: 0.8818 - val_loss: 1.1090\n",
      "Epoch 138/200\n",
      "907/907 [==============================] - 0s 387us/sample - loss: 0.8819 - val_loss: 1.1088\n",
      "Epoch 139/200\n",
      "907/907 [==============================] - 0s 372us/sample - loss: 0.8818 - val_loss: 1.1088\n",
      "Epoch 140/200\n",
      "907/907 [==============================] - 0s 391us/sample - loss: 0.8818 - val_loss: 1.1088\n",
      "Epoch 141/200\n",
      "907/907 [==============================] - 0s 386us/sample - loss: 0.8818 - val_loss: 1.1088\n",
      "Epoch 142/200\n",
      "907/907 [==============================] - 0s 384us/sample - loss: 0.8818 - val_loss: 1.1087\n",
      "Epoch 143/200\n",
      "907/907 [==============================] - 0s 472us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 144/200\n",
      "907/907 [==============================] - 0s 373us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 145/200\n",
      "907/907 [==============================] - 0s 389us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 146/200\n",
      "907/907 [==============================] - 0s 443us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 147/200\n",
      "907/907 [==============================] - 0s 400us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 148/200\n",
      "907/907 [==============================] - 0s 393us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 149/200\n",
      "907/907 [==============================] - 0s 395us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 150/200\n",
      "907/907 [==============================] - 0s 395us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 151/200\n",
      "907/907 [==============================] - 0s 387us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 152/200\n",
      "907/907 [==============================] - 0s 415us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 153/200\n",
      "907/907 [==============================] - 0s 388us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 154/200\n",
      "907/907 [==============================] - 0s 390us/sample - loss: 0.8817 - val_loss: 1.1088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 156/200\n",
      "907/907 [==============================] - 0s 395us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 157/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 158/200\n",
      "907/907 [==============================] - 0s 353us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 159/200\n",
      "907/907 [==============================] - 0s 350us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 160/200\n",
      "907/907 [==============================] - 0s 358us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 161/200\n",
      "907/907 [==============================] - 0s 368us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 162/200\n",
      "907/907 [==============================] - 1s 590us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 163/200\n",
      "907/907 [==============================] - 1s 896us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 164/200\n",
      "907/907 [==============================] - 1s 893us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 165/200\n",
      "907/907 [==============================] - 1s 909us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 166/200\n",
      "907/907 [==============================] - 1s 656us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 167/200\n",
      "907/907 [==============================] - 1s 847us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 168/200\n",
      "907/907 [==============================] - 1s 561us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 169/200\n",
      "907/907 [==============================] - 0s 511us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 170/200\n",
      "907/907 [==============================] - 1s 909us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 171/200\n",
      "907/907 [==============================] - 1s 658us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 172/200\n",
      "907/907 [==============================] - 0s 354us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 173/200\n",
      "907/907 [==============================] - 0s 365us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 174/200\n",
      "907/907 [==============================] - 0s 369us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 175/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 176/200\n",
      "907/907 [==============================] - 0s 364us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 177/200\n",
      "907/907 [==============================] - 0s 355us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 178/200\n",
      "907/907 [==============================] - 0s 359us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 179/200\n",
      "907/907 [==============================] - 0s 361us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 180/200\n",
      "907/907 [==============================] - 0s 358us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 181/200\n",
      "907/907 [==============================] - 0s 362us/sample - loss: 0.8817 - val_loss: 1.1088\n",
      "Epoch 182/200\n",
      "907/907 [==============================] - 0s 366us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 183/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 184/200\n",
      "907/907 [==============================] - 0s 362us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 185/200\n",
      "907/907 [==============================] - 0s 361us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 186/200\n",
      "907/907 [==============================] - 0s 365us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 187/200\n",
      "907/907 [==============================] - 0s 369us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 188/200\n",
      "907/907 [==============================] - 0s 367us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 189/200\n",
      "907/907 [==============================] - 0s 365us/sample - loss: 0.8817 - val_loss: 1.1087\n",
      "Epoch 190/200\n",
      "907/907 [==============================] - 0s 368us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 191/200\n",
      "907/907 [==============================] - 0s 402us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 192/200\n",
      "907/907 [==============================] - 0s 357us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 193/200\n",
      "907/907 [==============================] - 0s 381us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 194/200\n",
      "907/907 [==============================] - 0s 365us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 195/200\n",
      "907/907 [==============================] - 0s 366us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 196/200\n",
      "907/907 [==============================] - 0s 374us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 197/200\n",
      "907/907 [==============================] - 0s 370us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 198/200\n",
      "907/907 [==============================] - 0s 377us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 199/200\n",
      "907/907 [==============================] - 0s 373us/sample - loss: 0.8816 - val_loss: 1.1087\n",
      "Epoch 200/200\n",
      "907/907 [==============================] - 0s 383us/sample - loss: 0.8816 - val_loss: 1.1087\n"
     ]
    }
   ],
   "source": [
    "# learning schedule callback\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the MLP\n",
    "history = model.fit(X_train_centered, y_train, batch_size=16, epochs=200, verbose=1, callbacks=[callback], validation_split=0.1) # 90% training / 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 67.16%\n",
      "Test accuracy: 56.85%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
