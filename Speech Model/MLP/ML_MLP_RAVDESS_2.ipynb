{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************************************\n",
    "# Rezwan Matin\n",
    "# Thesis B\n",
    "# Filename: ML_MLP_RAVDESS_2.py\n",
    "# Date: 3/8/20\n",
    "#\n",
    "# Objective:\n",
    "# 26 MFCCs (mean) and 26 MFCCs (standard deviation), ZCR with Learning Scheduler!\n",
    "#\n",
    "#*************************************************************************************\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as rosa\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.48219694e-322, 1.41440670e-315, 1.38338381e-322, ...,\n",
       "       9.32265673e+000, 9.21448291e-002, 8.00000000e+000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save directory path in 'path'\n",
    "path = r'C:\\Books\\Texas State Books\\Fall 2019\\Thesis A\\Corpus\\Simulated\\RAVDESS\\All'\n",
    "\n",
    "# Declare a dummy Numpy array (row vector)\n",
    "result_array = np.empty([1,54])\n",
    "\n",
    "# Create a list of audio file names 'file_list'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "i=0\n",
    "\n",
    "for filename in file_list:\n",
    "    \n",
    "    # Read WAV file. 'rosa.core.load' returns sampling frequency in 'fs' and audio signal in 'sig'\n",
    "    sig, fs = rosa.core.load(path + '\\\\' + file_list[i], sr=None)\n",
    "    \n",
    "    # Calculate the average mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    avg_mfcc_feat = np.mean(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26).T,axis=0)\n",
    "    \n",
    "    # Calculate the standard deviation of mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.std' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    std_mfcc_feat = np.std(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26).T,axis=0)\n",
    "    \n",
    "    # Calculate the average zero crossing rate (utterance-level feature) using 'rosa.feat.zero_crossing_rate()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    zcross_feat = rosa.feature.zero_crossing_rate(sig)\n",
    "    avg_zcross_feat = np.mean(rosa.feature.zero_crossing_rate(y=sig).T,axis=0)\n",
    "    \n",
    "    # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "    feat0 = np.append(avg_mfcc_feat, std_mfcc_feat, axis=0)\n",
    "    \n",
    "    feat1 = np.append(feat0, avg_zcross_feat, axis=0)\n",
    "    \n",
    "    # Save emotion label from file name. 'path' contains directory's address, 'file_list' contains file name, and '\\\\' joins the two to form file's address\n",
    "    label = os.path.splitext(os.path.basename(path + '\\\\' + file_list[i]))[0].split('-')[2]\n",
    "    \n",
    "    # Create a new Numpy array 'sample' to store features along with label\n",
    "    sample = np.insert(feat1, obj=53, values=label)\n",
    "    \n",
    "    result_array = np.append(result_array, sample)\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# Print out the 1D Numpy array\n",
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77814,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 54)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 1D Numpy array to 2D array. Argument must be a Tuple. i+1 because we have i samples (audio files) plus a dummy row.\n",
    "result_array = np.reshape(result_array, (i+1,-1))\n",
    "\n",
    "# Delete first dummy row from 2D array\n",
    "result_array = np.delete(result_array, 0, 0)\n",
    "\n",
    "# Print final 2D Numpy array \n",
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>-596.489909</td>\n",
       "      <td>60.752540</td>\n",
       "      <td>1.096842</td>\n",
       "      <td>10.348159</td>\n",
       "      <td>-2.365429</td>\n",
       "      <td>7.986745</td>\n",
       "      <td>-2.027191</td>\n",
       "      <td>-0.116491</td>\n",
       "      <td>-6.654691</td>\n",
       "      <td>-1.406857</td>\n",
       "      <td>...</td>\n",
       "      <td>4.855516</td>\n",
       "      <td>7.889321</td>\n",
       "      <td>6.462217</td>\n",
       "      <td>6.308829</td>\n",
       "      <td>6.093730</td>\n",
       "      <td>4.775629</td>\n",
       "      <td>6.743428</td>\n",
       "      <td>4.953056</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>-625.565085</td>\n",
       "      <td>37.601565</td>\n",
       "      <td>-3.692515</td>\n",
       "      <td>8.979277</td>\n",
       "      <td>-5.637381</td>\n",
       "      <td>1.969832</td>\n",
       "      <td>-14.028974</td>\n",
       "      <td>1.621135</td>\n",
       "      <td>-13.249778</td>\n",
       "      <td>-2.752983</td>\n",
       "      <td>...</td>\n",
       "      <td>8.569099</td>\n",
       "      <td>9.995390</td>\n",
       "      <td>9.139846</td>\n",
       "      <td>7.003860</td>\n",
       "      <td>8.466257</td>\n",
       "      <td>6.507719</td>\n",
       "      <td>6.422495</td>\n",
       "      <td>7.017977</td>\n",
       "      <td>0.100822</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>-589.096402</td>\n",
       "      <td>81.593995</td>\n",
       "      <td>-0.673410</td>\n",
       "      <td>12.050125</td>\n",
       "      <td>10.394698</td>\n",
       "      <td>19.574634</td>\n",
       "      <td>-4.826030</td>\n",
       "      <td>2.336833</td>\n",
       "      <td>-1.092758</td>\n",
       "      <td>-2.186526</td>\n",
       "      <td>...</td>\n",
       "      <td>7.340301</td>\n",
       "      <td>6.230392</td>\n",
       "      <td>6.443201</td>\n",
       "      <td>8.798983</td>\n",
       "      <td>4.902457</td>\n",
       "      <td>4.589814</td>\n",
       "      <td>7.527595</td>\n",
       "      <td>4.950332</td>\n",
       "      <td>0.060662</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>-567.861314</td>\n",
       "      <td>50.748331</td>\n",
       "      <td>-6.996325</td>\n",
       "      <td>2.507971</td>\n",
       "      <td>-4.691477</td>\n",
       "      <td>0.188368</td>\n",
       "      <td>-15.399068</td>\n",
       "      <td>-8.173095</td>\n",
       "      <td>-10.797020</td>\n",
       "      <td>-5.656966</td>\n",
       "      <td>...</td>\n",
       "      <td>11.171980</td>\n",
       "      <td>8.206702</td>\n",
       "      <td>7.901195</td>\n",
       "      <td>8.828436</td>\n",
       "      <td>9.154752</td>\n",
       "      <td>9.932455</td>\n",
       "      <td>9.488486</td>\n",
       "      <td>9.067261</td>\n",
       "      <td>0.071194</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>-493.306198</td>\n",
       "      <td>48.233703</td>\n",
       "      <td>-3.028220</td>\n",
       "      <td>18.852342</td>\n",
       "      <td>-3.590502</td>\n",
       "      <td>9.721251</td>\n",
       "      <td>-2.724754</td>\n",
       "      <td>6.184319</td>\n",
       "      <td>-1.879220</td>\n",
       "      <td>2.796708</td>\n",
       "      <td>...</td>\n",
       "      <td>8.423227</td>\n",
       "      <td>8.431807</td>\n",
       "      <td>10.100727</td>\n",
       "      <td>6.233350</td>\n",
       "      <td>5.368571</td>\n",
       "      <td>6.040829</td>\n",
       "      <td>8.945960</td>\n",
       "      <td>9.681306</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>-615.406134</td>\n",
       "      <td>43.642912</td>\n",
       "      <td>-6.402692</td>\n",
       "      <td>12.012579</td>\n",
       "      <td>-11.143824</td>\n",
       "      <td>5.057568</td>\n",
       "      <td>-10.034647</td>\n",
       "      <td>9.767619</td>\n",
       "      <td>-5.317049</td>\n",
       "      <td>-4.267572</td>\n",
       "      <td>...</td>\n",
       "      <td>8.939621</td>\n",
       "      <td>6.847688</td>\n",
       "      <td>8.352500</td>\n",
       "      <td>6.104008</td>\n",
       "      <td>9.562205</td>\n",
       "      <td>9.682800</td>\n",
       "      <td>7.298084</td>\n",
       "      <td>11.122962</td>\n",
       "      <td>0.062682</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>-696.193632</td>\n",
       "      <td>72.680527</td>\n",
       "      <td>6.986117</td>\n",
       "      <td>24.891391</td>\n",
       "      <td>2.902952</td>\n",
       "      <td>9.812980</td>\n",
       "      <td>1.004795</td>\n",
       "      <td>9.548923</td>\n",
       "      <td>-5.149150</td>\n",
       "      <td>-1.495172</td>\n",
       "      <td>...</td>\n",
       "      <td>6.992092</td>\n",
       "      <td>7.291586</td>\n",
       "      <td>6.982269</td>\n",
       "      <td>7.322814</td>\n",
       "      <td>5.810611</td>\n",
       "      <td>5.193900</td>\n",
       "      <td>8.329027</td>\n",
       "      <td>4.884544</td>\n",
       "      <td>0.053795</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>-615.786554</td>\n",
       "      <td>53.243328</td>\n",
       "      <td>-18.664904</td>\n",
       "      <td>4.902306</td>\n",
       "      <td>-5.074936</td>\n",
       "      <td>2.717509</td>\n",
       "      <td>-12.459353</td>\n",
       "      <td>0.465174</td>\n",
       "      <td>-15.718720</td>\n",
       "      <td>-9.009732</td>\n",
       "      <td>...</td>\n",
       "      <td>8.659487</td>\n",
       "      <td>9.041492</td>\n",
       "      <td>9.361286</td>\n",
       "      <td>8.517360</td>\n",
       "      <td>8.430611</td>\n",
       "      <td>9.607357</td>\n",
       "      <td>7.680336</td>\n",
       "      <td>10.634757</td>\n",
       "      <td>0.073165</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>-540.863667</td>\n",
       "      <td>61.330114</td>\n",
       "      <td>-9.318194</td>\n",
       "      <td>14.311654</td>\n",
       "      <td>0.700296</td>\n",
       "      <td>10.042510</td>\n",
       "      <td>-8.720129</td>\n",
       "      <td>-2.253887</td>\n",
       "      <td>-3.264636</td>\n",
       "      <td>-8.769154</td>\n",
       "      <td>...</td>\n",
       "      <td>6.450383</td>\n",
       "      <td>7.456291</td>\n",
       "      <td>5.409208</td>\n",
       "      <td>5.574703</td>\n",
       "      <td>5.130295</td>\n",
       "      <td>6.740843</td>\n",
       "      <td>6.362919</td>\n",
       "      <td>4.742207</td>\n",
       "      <td>0.069006</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>-560.186503</td>\n",
       "      <td>44.504401</td>\n",
       "      <td>-14.639301</td>\n",
       "      <td>2.251491</td>\n",
       "      <td>-11.031190</td>\n",
       "      <td>5.346806</td>\n",
       "      <td>-10.163466</td>\n",
       "      <td>-6.553192</td>\n",
       "      <td>-9.599661</td>\n",
       "      <td>-7.118306</td>\n",
       "      <td>...</td>\n",
       "      <td>8.714345</td>\n",
       "      <td>12.087737</td>\n",
       "      <td>11.294306</td>\n",
       "      <td>11.975909</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>10.152889</td>\n",
       "      <td>10.284631</td>\n",
       "      <td>8.092669</td>\n",
       "      <td>0.083883</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>-549.112537</td>\n",
       "      <td>51.645593</td>\n",
       "      <td>-13.533260</td>\n",
       "      <td>5.057537</td>\n",
       "      <td>-7.751437</td>\n",
       "      <td>6.483627</td>\n",
       "      <td>-8.185426</td>\n",
       "      <td>-3.678960</td>\n",
       "      <td>-7.495473</td>\n",
       "      <td>-5.939384</td>\n",
       "      <td>...</td>\n",
       "      <td>6.832755</td>\n",
       "      <td>7.636967</td>\n",
       "      <td>9.260758</td>\n",
       "      <td>6.784180</td>\n",
       "      <td>6.211125</td>\n",
       "      <td>5.630223</td>\n",
       "      <td>8.386614</td>\n",
       "      <td>10.673716</td>\n",
       "      <td>0.075679</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>-536.693773</td>\n",
       "      <td>43.281145</td>\n",
       "      <td>-9.283843</td>\n",
       "      <td>9.252940</td>\n",
       "      <td>-10.546879</td>\n",
       "      <td>3.863054</td>\n",
       "      <td>-11.571782</td>\n",
       "      <td>-4.176649</td>\n",
       "      <td>-9.068328</td>\n",
       "      <td>-4.904865</td>\n",
       "      <td>...</td>\n",
       "      <td>10.236763</td>\n",
       "      <td>9.565846</td>\n",
       "      <td>11.509971</td>\n",
       "      <td>10.812383</td>\n",
       "      <td>9.733490</td>\n",
       "      <td>10.543449</td>\n",
       "      <td>9.538634</td>\n",
       "      <td>9.322657</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "1236 -596.489909  60.752540   1.096842  10.348159  -2.365429   7.986745   \n",
       "1237 -625.565085  37.601565  -3.692515   8.979277  -5.637381   1.969832   \n",
       "1238 -589.096402  81.593995  -0.673410  12.050125  10.394698  19.574634   \n",
       "1239 -567.861314  50.748331  -6.996325   2.507971  -4.691477   0.188368   \n",
       "1240 -493.306198  48.233703  -3.028220  18.852342  -3.590502   9.721251   \n",
       "1241 -615.406134  43.642912  -6.402692  12.012579 -11.143824   5.057568   \n",
       "1242 -696.193632  72.680527   6.986117  24.891391   2.902952   9.812980   \n",
       "1243 -615.786554  53.243328 -18.664904   4.902306  -5.074936   2.717509   \n",
       "1244 -540.863667  61.330114  -9.318194  14.311654   0.700296  10.042510   \n",
       "1245 -560.186503  44.504401 -14.639301   2.251491 -11.031190   5.346806   \n",
       "1246 -549.112537  51.645593 -13.533260   5.057537  -7.751437   6.483627   \n",
       "1247 -536.693773  43.281145  -9.283843   9.252940 -10.546879   3.863054   \n",
       "\n",
       "              6         7          8         9  ...         44         45  \\\n",
       "1236  -2.027191 -0.116491  -6.654691 -1.406857  ...   4.855516   7.889321   \n",
       "1237 -14.028974  1.621135 -13.249778 -2.752983  ...   8.569099   9.995390   \n",
       "1238  -4.826030  2.336833  -1.092758 -2.186526  ...   7.340301   6.230392   \n",
       "1239 -15.399068 -8.173095 -10.797020 -5.656966  ...  11.171980   8.206702   \n",
       "1240  -2.724754  6.184319  -1.879220  2.796708  ...   8.423227   8.431807   \n",
       "1241 -10.034647  9.767619  -5.317049 -4.267572  ...   8.939621   6.847688   \n",
       "1242   1.004795  9.548923  -5.149150 -1.495172  ...   6.992092   7.291586   \n",
       "1243 -12.459353  0.465174 -15.718720 -9.009732  ...   8.659487   9.041492   \n",
       "1244  -8.720129 -2.253887  -3.264636 -8.769154  ...   6.450383   7.456291   \n",
       "1245 -10.163466 -6.553192  -9.599661 -7.118306  ...   8.714345  12.087737   \n",
       "1246  -8.185426 -3.678960  -7.495473 -5.939384  ...   6.832755   7.636967   \n",
       "1247 -11.571782 -4.176649  -9.068328 -4.904865  ...  10.236763   9.565846   \n",
       "\n",
       "             46         47         48         49         50         51  \\\n",
       "1236   6.462217   6.308829   6.093730   4.775629   6.743428   4.953056   \n",
       "1237   9.139846   7.003860   8.466257   6.507719   6.422495   7.017977   \n",
       "1238   6.443201   8.798983   4.902457   4.589814   7.527595   4.950332   \n",
       "1239   7.901195   8.828436   9.154752   9.932455   9.488486   9.067261   \n",
       "1240  10.100727   6.233350   5.368571   6.040829   8.945960   9.681306   \n",
       "1241   8.352500   6.104008   9.562205   9.682800   7.298084  11.122962   \n",
       "1242   6.982269   7.322814   5.810611   5.193900   8.329027   4.884544   \n",
       "1243   9.361286   8.517360   8.430611   9.607357   7.680336  10.634757   \n",
       "1244   5.409208   5.574703   5.130295   6.740843   6.362919   4.742207   \n",
       "1245  11.294306  11.975909  15.346170  10.152889  10.284631   8.092669   \n",
       "1246   9.260758   6.784180   6.211125   5.630223   8.386614  10.673716   \n",
       "1247  11.509971  10.812383   9.733490  10.543449   9.538634   9.322657   \n",
       "\n",
       "            52  Emotion  \n",
       "1236  0.066450      8.0  \n",
       "1237  0.100822      8.0  \n",
       "1238  0.060662      8.0  \n",
       "1239  0.071194      8.0  \n",
       "1240  0.068974      8.0  \n",
       "1241  0.062682      8.0  \n",
       "1242  0.053795      8.0  \n",
       "1243  0.073165      8.0  \n",
       "1244  0.069006      8.0  \n",
       "1245  0.083883      8.0  \n",
       "1246  0.075679      8.0  \n",
       "1247  0.092145      8.0  \n",
       "\n",
       "[12 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=result_array)\n",
    "# Label only the last (target) column\n",
    "df = df.rename({53: \"Emotion\"}, axis='columns')\n",
    "# Delete unnecessary emotion data (calm)\n",
    "df.drop(df[df['Emotion'] == 2.0].index, inplace = True)\n",
    "# Reset row (sample) indexing\n",
    "df = df.reset_index(drop=True)\n",
    "df.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0    192\n",
       "7.0    192\n",
       "6.0    192\n",
       "5.0    192\n",
       "4.0    192\n",
       "3.0    192\n",
       "1.0     96\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0    192\n",
       "7.0    192\n",
       "6.0    192\n",
       "5.0    192\n",
       "4.0    192\n",
       "3.0    192\n",
       "1.0    192\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the dataset for equal number of samples for each class.\n",
    "# Separate majority and minority classes\n",
    "df_minority = df[df.Emotion==1.0]\n",
    "df_majority3 = df[df.Emotion==3.0]\n",
    "df_majority4 = df[df.Emotion==4.0]\n",
    "df_majority5 = df[df.Emotion==5.0]\n",
    "df_majority6 = df[df.Emotion==6.0]\n",
    "df_majority7 = df[df.Emotion==7.0]\n",
    "df_majority8 = df[df.Emotion==8.0]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=192,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_minority_upsampled, df_majority3, df_majority4, df_majority5, df_majority6, df_majority7, df_majority8])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.Emotion.value_counts()\n",
    "\n",
    "# Reset row (sample) indexing\n",
    "df_upsampled = df_upsampled.reset_index(drop=True)\n",
    "\n",
    "df_upsampled['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 8. 8. 8.]\n"
     ]
    }
   ],
   "source": [
    "# Extract target feature 'Emotion' in a vector y. Indexing from 0\n",
    "y = df_upsampled.iloc[0:1344, 53].values\n",
    "# Extract features 'buying' and 'safety' in a vector X. Indexing from 0\n",
    "X = df_upsampled.iloc[0:1344, list(range(53))].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 53) (1008,)\n",
      "(336, 53) (336,)\n"
     ]
    }
   ],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "# Standardize the inputs\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# One-Hot Encode the classes\n",
    "y_train_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler which reduces learning rate to half every 10 epochs\n",
    "def scheduler(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object/instance 'model' for the 'Sequential()' class.\n",
    "model = keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=X_train_centered.shape[1],\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros', \n",
    "                activation='relu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='relu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='relu'))\n",
    "model.add(\n",
    "    keras.layers.Dense( units=53,\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='relu'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense( units=y_train_onehot.shape[1],\n",
    "                input_dim=53,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "                    lr=0.0, decay=1e-7, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "                    loss='categorical_crossentropy')\n",
    "                          \n",
    "                            # cross-entropy: fancy name for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 907 samples, validate on 101 samples\n",
      "Epoch 1/50\n",
      "907/907 [==============================] - 0s 315us/sample - loss: 2.0395 - val_loss: 1.8272\n",
      "Epoch 2/50\n",
      "907/907 [==============================] - 0s 62us/sample - loss: 1.8023 - val_loss: 1.7750\n",
      "Epoch 3/50\n",
      "907/907 [==============================] - 0s 73us/sample - loss: 1.7488 - val_loss: 1.7290\n",
      "Epoch 4/50\n",
      "907/907 [==============================] - 0s 67us/sample - loss: 1.7286 - val_loss: 1.7000\n",
      "Epoch 5/50\n",
      "907/907 [==============================] - 0s 64us/sample - loss: 1.7060 - val_loss: 1.6508\n",
      "Epoch 6/50\n",
      "907/907 [==============================] - 0s 79us/sample - loss: 1.7487 - val_loss: 1.6540\n",
      "Epoch 7/50\n",
      "907/907 [==============================] - 0s 75us/sample - loss: 1.7310 - val_loss: 1.6664\n",
      "Epoch 8/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 1.7435 - val_loss: 1.6875\n",
      "Epoch 9/50\n",
      "907/907 [==============================] - 0s 69us/sample - loss: 1.6691 - val_loss: 1.6020\n",
      "Epoch 10/50\n",
      "907/907 [==============================] - 0s 67us/sample - loss: 1.6301 - val_loss: 1.5871\n",
      "Epoch 11/50\n",
      "907/907 [==============================] - 0s 121us/sample - loss: 1.5945 - val_loss: 1.5602\n",
      "Epoch 12/50\n",
      "907/907 [==============================] - 0s 67us/sample - loss: 1.6131 - val_loss: 1.5349\n",
      "Epoch 13/50\n",
      "907/907 [==============================] - 0s 71us/sample - loss: 1.5500 - val_loss: 1.4887\n",
      "Epoch 14/50\n",
      "907/907 [==============================] - 0s 75us/sample - loss: 1.5864 - val_loss: 1.5082\n",
      "Epoch 15/50\n",
      "907/907 [==============================] - 0s 72us/sample - loss: 1.5244 - val_loss: 1.5055\n",
      "Epoch 16/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 1.4976 - val_loss: 1.5111\n",
      "Epoch 17/50\n",
      "907/907 [==============================] - 0s 76us/sample - loss: 1.5070 - val_loss: 1.4690\n",
      "Epoch 18/50\n",
      "907/907 [==============================] - 0s 80us/sample - loss: 1.5482 - val_loss: 1.5044\n",
      "Epoch 19/50\n",
      "907/907 [==============================] - 0s 104us/sample - loss: 1.4693 - val_loss: 1.4113\n",
      "Epoch 20/50\n",
      "907/907 [==============================] - 0s 101us/sample - loss: 1.4170 - val_loss: 1.5745\n",
      "Epoch 21/50\n",
      "907/907 [==============================] - 0s 114us/sample - loss: 1.4137 - val_loss: 1.3865\n",
      "Epoch 22/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 1.4140 - val_loss: 1.3210\n",
      "Epoch 23/50\n",
      "907/907 [==============================] - 0s 92us/sample - loss: 1.3878 - val_loss: 1.3606\n",
      "Epoch 24/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.3202 - val_loss: 1.3024\n",
      "Epoch 25/50\n",
      "907/907 [==============================] - 0s 65us/sample - loss: 1.2991 - val_loss: 1.2981\n",
      "Epoch 26/50\n",
      "907/907 [==============================] - 0s 92us/sample - loss: 1.3111 - val_loss: 1.3297\n",
      "Epoch 27/50\n",
      "907/907 [==============================] - 0s 62us/sample - loss: 1.2748 - val_loss: 1.2928\n",
      "Epoch 28/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.2355 - val_loss: 1.3298\n",
      "Epoch 29/50\n",
      "907/907 [==============================] - 0s 73us/sample - loss: 1.2359 - val_loss: 1.2546\n",
      "Epoch 30/50\n",
      "907/907 [==============================] - 0s 67us/sample - loss: 1.2297 - val_loss: 1.2384\n",
      "Epoch 31/50\n",
      "907/907 [==============================] - 0s 68us/sample - loss: 1.2125 - val_loss: 1.2085\n",
      "Epoch 32/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.2140 - val_loss: 1.3615\n",
      "Epoch 33/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.2221 - val_loss: 1.3010\n",
      "Epoch 34/50\n",
      "907/907 [==============================] - 0s 115us/sample - loss: 1.2129 - val_loss: 1.2475\n",
      "Epoch 35/50\n",
      "907/907 [==============================] - 0s 100us/sample - loss: 1.1906 - val_loss: 1.2612\n",
      "Epoch 36/50\n",
      "907/907 [==============================] - 0s 71us/sample - loss: 1.1531 - val_loss: 1.2507\n",
      "Epoch 37/50\n",
      "907/907 [==============================] - 0s 63us/sample - loss: 1.1335 - val_loss: 1.2581\n",
      "Epoch 38/50\n",
      "907/907 [==============================] - 0s 60us/sample - loss: 1.1367 - val_loss: 1.2308\n",
      "Epoch 39/50\n",
      "907/907 [==============================] - 0s 66us/sample - loss: 1.1820 - val_loss: 1.2034\n",
      "Epoch 40/50\n",
      "907/907 [==============================] - 0s 77us/sample - loss: 1.1841 - val_loss: 1.2462\n",
      "Epoch 41/50\n",
      "907/907 [==============================] - 0s 82us/sample - loss: 1.1203 - val_loss: 1.2366\n",
      "Epoch 42/50\n",
      "907/907 [==============================] - 0s 62us/sample - loss: 1.0991 - val_loss: 1.2066\n",
      "Epoch 43/50\n",
      "907/907 [==============================] - 0s 95us/sample - loss: 1.0819 - val_loss: 1.2200\n",
      "Epoch 44/50\n",
      "907/907 [==============================] - 0s 111us/sample - loss: 1.0761 - val_loss: 1.2074\n",
      "Epoch 45/50\n",
      "907/907 [==============================] - 0s 83us/sample - loss: 1.0770 - val_loss: 1.2080\n",
      "Epoch 46/50\n",
      "907/907 [==============================] - 0s 64us/sample - loss: 1.0684 - val_loss: 1.1860\n",
      "Epoch 47/50\n",
      "907/907 [==============================] - 0s 76us/sample - loss: 1.0870 - val_loss: 1.2847\n",
      "Epoch 48/50\n",
      "907/907 [==============================] - 0s 89us/sample - loss: 1.0803 - val_loss: 1.2246\n",
      "Epoch 49/50\n",
      "907/907 [==============================] - 0s 66us/sample - loss: 1.0557 - val_loss: 1.1817\n",
      "Epoch 50/50\n",
      "907/907 [==============================] - 0s 77us/sample - loss: 1.0660 - val_loss: 1.2790\n"
     ]
    }
   ],
   "source": [
    "# learning schedule callback\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the MLP\n",
    "history = model.fit(X_train_centered, y_train_onehot, batch_size=64, epochs=50, verbose=1, callbacks=[callback], validation_split=0.1) # 90% training / 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 59.23%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 58.93%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
