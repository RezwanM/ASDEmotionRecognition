{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maleeha\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜ž  :  Sad\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜   :  Angry\n",
      "ðŸ˜¨  :  Fearful\n",
      "ðŸ˜   :  Angry\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-01c5ab21f058>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplerate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0msd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Wait until recording is finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m48000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sounddevice.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(ignore_errors)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \"\"\"\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_last_callback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_last_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\sounddevice.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, ignore_errors)\u001b[0m\n\u001b[0;32m   2590\u001b[0m         \"\"\"\n\u001b[0;32m   2591\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2592\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2593\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2594\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#************************************************************************************\n",
    "# Rezwan Matin\n",
    "# Thesis B\n",
    "# Filename: All_Librosa_Comple_Clean_Load.py\n",
    "# Date: 10/22/20\n",
    "#\n",
    "# Features:\n",
    "# 26 MFCCs, 7 spectral contrast, 2 poly features, and 1 RMS.\n",
    "#\n",
    "#*************************************************************************************\n",
    "\n",
    "\n",
    "import librosa as rosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import sounddevice as sd\n",
    "import statistics\n",
    "import emoji\n",
    "from statistics import StatisticsError\n",
    "\n",
    "\n",
    "# Load the SVM model from the pickle file \n",
    "svm_pkl = joblib.load('SVM_Librosa_Complete_Clean.pkl')  \n",
    "\n",
    "# Load the MLP model from the pickle file \n",
    "mlp_h5 = tf.keras.models.load_model('MLP_Librosa_Complete_Clean.h5')\n",
    "\n",
    "# Load the RNN model from the pickle file \n",
    "rnn_h5 = tf.keras.models.load_model('RNN_Librosa_Complete_Clean.h5') \n",
    "\n",
    "# Load the arrays containing means and standard deviations of features from training for SVM and MLP model\n",
    "mean_vals = np.load('mean_feat_librosa_clean.npy')\n",
    "std_val = np.load('std_feat_librosa_clean.npy')\n",
    "\n",
    "# Load the arrays containing means and standard deviations of features from training for SVM and MLP model\n",
    "mean_vals_rnn = np.load('mean_feat_rnn_librosa_clean.npy')\n",
    "std_val_rnn = np.load('std_feat_rnn_librosa_clean.npy')\n",
    "\n",
    "fs = 16000  # Record at 16000 samples per second\n",
    "seconds = 3\n",
    "median_num_frames = 153 # 16000*3/512\n",
    "\n",
    "def change_label(argument):\n",
    "    switcher = {\n",
    "        1:\"Neutral\",\n",
    "        2:\"Happy\",\n",
    "        3:\"Sad\",\n",
    "        4:\"Angry\",\n",
    "        5:\"Fearful\",\n",
    "        6:\"Disgust\",\n",
    "        7:\"Surprised\",\n",
    "    }\n",
    "    return switcher.get(argument, \"Nothing\")\n",
    "\n",
    "def label_emoji(argument):\n",
    "    switcher = {\n",
    "        1:emoji.emojize(\":neutral_face:\"),\n",
    "        2:emoji.emojize(\":grinning_face_with_smiling_eyes:\"),\n",
    "        3:emoji.emojize(\":disappointed_face:\"),\n",
    "        4:emoji.emojize(\":angry_face:\"),\n",
    "        5:emoji.emojize(\":fearful_face:\"),\n",
    "        6:emoji.emojize(\":face_vomiting:\"),\n",
    "        7:emoji.emojize(\":hushed_face:\"),\n",
    "    }\n",
    "    return switcher.get(argument, \"Nothing\")\n",
    "\n",
    "print('Recording...')\n",
    "\n",
    "while True:\n",
    "    # In sounddevice, frames mean samples!\n",
    "    # Blocksize is the number of samples per frame!\n",
    "    \n",
    "    # Store recorded signal into a Numpy array\n",
    "    sig = sd.rec(frames=int(fs*seconds), samplerate=fs, channels=1, blocksize=512)\n",
    "    \n",
    "    sd.wait() # Wait until recording is finished\n",
    "    \n",
    "    sig = np.reshape(sig, (48000,))\n",
    "    \n",
    "    # SVM and MLP feature extraction\n",
    "    # Calculate the average mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.mean' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    avg_mfcc_feat = np.mean(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26, n_fft=512, hop_length=256, htk=True).T,axis=0)\n",
    "    \n",
    "    # Calculate the standard deviation of mfcc (utterance-level features) using 'rosa.feat.mfcc()' and 'np.std' method. '.T' transposes the rows and columns. 'axis=0' indicates average is calculated column-wise\n",
    "    std_mfcc_feat = np.std(rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26, n_fft=512, hop_length=256, htk=True).T,axis=0)\n",
    "    \n",
    "    avg_spec_feat = np.mean(rosa.feature.spectral_contrast(y=sig, sr=fs, n_fft=512, hop_length=256).T, axis=0)\n",
    "    \n",
    "    avg_poly_feat = np.mean(rosa.feature.poly_features(y=sig, sr=fs, n_fft=512, hop_length=256).T, axis=0)\n",
    "    \n",
    "    avg_rms_feat = np.mean(rosa.feature.rms(y=sig, frame_length=512, hop_length=256).T, axis=0)\n",
    "    \n",
    "    # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "    feat0 = np.append(avg_mfcc_feat, std_mfcc_feat, axis=0)\n",
    "    \n",
    "    feat1 = np.append(feat0, avg_spec_feat, axis=0)\n",
    "    \n",
    "    feat2 = np.append(feat1, avg_poly_feat, axis=0)\n",
    "    \n",
    "    feat3 = np.append(feat2, avg_rms_feat, axis=0)\n",
    "    \n",
    "    feat = np.reshape(feat3, (1,-1)) \n",
    "\n",
    "    # Standardize the inputs means and standard deviations of features from training for SVM and MLP model\n",
    "    feat_centered = (feat - mean_vals)/std_val\n",
    "    \n",
    "    # RNN feature extraction\n",
    "    # 'rosa.feature.mfcc' extracts n_mfccs from signal and stores it into 'mfcc_feat'\n",
    "    mfcc_feat = rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26, n_fft=512, hop_length=256, htk=True)\n",
    "    \n",
    "    spec_feat = rosa.feature.spectral_contrast(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "\t\n",
    "    poly_feat = rosa.feature.poly_features(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "\t\n",
    "    rms_feat = rosa.feature.rms(y=sig, frame_length=512, hop_length=256)\n",
    "    \n",
    "    # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "    feat0 = np.append(mfcc_feat, spec_feat, axis=0)\n",
    "    \n",
    "    feat1 = np.append(feat0, poly_feat, axis=0)\n",
    "\t\n",
    "    feat2 = np.append(feat1, rms_feat, axis=0)\n",
    "    \n",
    "    # Transpose the array to flip the rows and columns. This is done so that the features become column parameters, making each row an audio frame.\n",
    "    transp_feat = feat2.T\n",
    "    \n",
    "    # Note: The 'cap frame number' is basically the limit we set for the number of frames for each sample, so that all samples have equal dimensions.\n",
    "    if transp_feat.shape[0]<median_num_frames:\n",
    "\n",
    "        # If number of frames is smaller than the cap frame number, we pad the array in order to reach our desired dimensions.\n",
    "\n",
    "        # Pad the array so that it matches the cap frame number. The second value in the argument contains two tuples which indicate which way to pad how much.  \n",
    "        transp_feat = np.pad(transp_feat, ((0, median_num_frames-transp_feat.shape[0]), (0,0)), 'mean')\n",
    "\n",
    "    elif transp_feat.shape[0]>median_num_frames:\n",
    "\n",
    "        # If number of frames is larger than the cap frame number, we delete rows (frames) which exceed the cap frame number in order to reach our desired dimensions.\n",
    "\n",
    "        # Define a tuple which contains the range of the row indices to delete.\n",
    "        row_del_index = (range(median_num_frames, transp_feat.shape[0], 1))\n",
    "\n",
    "        transp_feat = np.delete(transp_feat, row_del_index, axis=0)\n",
    "\n",
    "    else:\n",
    "        # If number of frames match the cap frame length, perfect!\n",
    "        transp_feat = transp_feat\n",
    "    \n",
    "    # Transpose again to flip the rows and columns. This is done so that the features become row parameters, making each column an audio frame.\n",
    "    transp2_feat = transp_feat.T\n",
    "    \n",
    "    # Flatten the entire 2D Numpy array into 1D Numpy array. So, the first 36 values of the 1D array represent the features for first frame, the second 36 represent the features for second frame, and so on till the final (cap) frame.\n",
    "    # 'C' means row-major ordered flattening.\n",
    "    feat_rnn = transp2_feat.flatten('C')\n",
    "    \n",
    "    feat_rnn = np.reshape(feat_rnn, (1,-1)) \n",
    "\n",
    "\t# Standardize the inputs means and standard deviations of features from training for RNN model\n",
    "    feat_centered_rnn = (feat_rnn - mean_vals_rnn)/std_val_rnn\n",
    "    \n",
    "    # Reshaping feat_centered to 3D Numpy array for feeding into the RNN. RNNs require 3D array input.\n",
    "    # 3D dimensions are (layers, rows, columns).\n",
    "    feat_3D = np.reshape(feat_centered_rnn, (feat_centered_rnn.shape[0], median_num_frames, 36))\n",
    "    \n",
    "    # Transpose tensors so that rows=features and columns=frames.\n",
    "    feat_3D_posed = tf.transpose(feat_3D, perm=[0, 2, 1])\n",
    "    \n",
    "    # Make prediction using SVM model\n",
    "    pred_svm = svm_pkl.predict(feat_centered)\n",
    "    \n",
    "    # Make prediction using MLP model\n",
    "    pred_mlp = mlp_h5.predict(feat_centered)\n",
    "    \n",
    "    # Convert One Hot label to integer label\n",
    "    pred_mlp = np.argmax(pred_mlp, axis=1)\n",
    "    \n",
    "    # Make prediction using RNN model\n",
    "    pred_rnn = rnn_h5.predict(feat_3D_posed)\n",
    "    \n",
    "    # Convert One Hot label to integer label\n",
    "    pred_rnn = np.argmax(pred_rnn, axis=1)\n",
    "    \n",
    "    # Put all three predictions in a list\n",
    "    preds = []\n",
    "    preds.append(int(pred_svm))\n",
    "    preds.append(int(pred_mlp))\n",
    "    preds.append(int(pred_rnn))\n",
    "    \n",
    "    # Voting - final prediction is the class that was predicted the most\n",
    "    try:\n",
    "        pred = statistics.mode(preds) # calculate mode of predicitions\n",
    "    except StatisticsError: \n",
    "        pred = int(pred_mlp) # if all unique predictions (no mode), select MLP's prediction\n",
    "    \n",
    "    emotion = change_label(pred)\n",
    "    \n",
    "    smiley = label_emoji(pred)\n",
    "    \n",
    "    print(smiley, \" : \", emotion)\n",
    "\n",
    "    del sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 0, 0, 0]]\n",
    "print(np.argmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
